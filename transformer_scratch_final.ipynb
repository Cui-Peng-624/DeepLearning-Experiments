{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce875ae1-d9ef-421f-adab-260bab9c4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from load_translation_data import load_data_nmt\n",
    "from visualization import TrainingVisualizer\n",
    "from transformer import PositionalEncoding,AddNorm,PositionWiseFFN,grad_clipping,sequence_mask,MaskedSoftmaxCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5784abe7-6e94-4911-98f2-b049fdb9804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f40c069-c53f-4a17-90cd-e615c5a7d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "\n",
    "batch_size = 64\n",
    "seq_size = 50\n",
    "\n",
    "normalized_shape = [num_hiddens]\n",
    "\n",
    "ffn_num_hiddens = 128\n",
    "num_layers = 6\n",
    "\n",
    "i = 0 \n",
    "\n",
    "num_epochs = 100\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dbf94b3-0dc4-4176-ae2c-1c108bb2a22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(184, 201)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, seq_size)\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "src_vocab_size, tgt_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06525da4-15c7-4956-b21f-6d00ded4a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size，seq_size，#_size*num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1) # (batch_size，seq_size，num_heads，#_size) \n",
    "    X = X.permute(0, 2, 1, 3) # (batch_size，num_heads，seq_size, #_size)\n",
    "    X = X.reshape(-1, X.shape[2], X.shape[3]) # 最终输出的形状: (batch_size*num_heads, seq_size, #_size)\n",
    "    return X\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    # 输出维度：(batch_size*num_heads, seq_size, value_size)\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2]) # (batch_size, num_heads, seq_size, value_size)\n",
    "    X = X.permute(0, 2, 1, 3) # (batch_size, seq_size, num_heads, value_size)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], -1) # (batch_size, seq_size, num_heads*value_size)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c2fe279-f24e-45db-8a29-429058fd4296",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs): \n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.key_size = num_hiddens / num_heads\n",
    "        self.W_q = nn.Linear(num_hiddens, num_hiddens, bias=bias) # 实际上：(embedding_size, query_size*num_heads)\n",
    "        self.W_k = nn.Linear(num_hiddens, num_hiddens, bias=bias) # 实际上：(embedding_size, key_size*num_heads)\n",
    "        self.W_v = nn.Linear(num_hiddens, num_hiddens, bias=bias) # 实际上：(embedding_size, value_size*num_heads)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias) # 实际上：(value_size*num_heads, output_size)\n",
    "\n",
    "    def forward(self, X_queries, X_keys, X_values, valid_lens=None):\n",
    "        queries = self.W_q(X_queries) # (batch_size, seq_size, query_size*num_heads)\n",
    "        keys = self.W_k(X_keys) # (batch_size, seq_size, key_size*num_heads)\n",
    "        values = self.W_v(X_values) # (batch_size, seq_size, value_size*num_heads)\n",
    "\n",
    "        queries = transpose_qkv(queries, self.num_heads) # (batch_size*num_heads, seq_size, query_size)\n",
    "        keys = transpose_qkv(keys, self.num_heads) # (batch_size*num_heads, seq_size, key_size)\n",
    "        values = transpose_qkv(values, self.num_heads) # (batch_size*num_heads, seq_size, value_size)\n",
    "\n",
    "        keys = keys.transpose(1, 2) # 交换keys的第一个和第二个维度（从0开始） -> (batch_size*num_heads, key_size, seq_size)\n",
    "        scores = torch.bmm(queries, keys) # 1.输出维度：(batch_size*num_heads, seq_size, seq_size) 2.我们需要保证 query_size = key_size！ 3.在decoder中是(batch_size*num_heads, seq_size, seq_size*(i+1))\n",
    "        scores = scores / torch.tensor([math.sqrt(self.key_size)]).to(scores.device) # 除以 根号下(key_size)；输出维度：(batch_size*num_heads, seq_size, seq_size)\n",
    "        \n",
    "        attention_weights = self.masked_softmax(scores, valid_lens)\n",
    "        # print(attention_weights, \"####################\\n\")\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        attention_outputs = torch.bmm(attention_weights, values) # (batch_size*num_heads, seq_size, seq_size) 与 (batch_size*num_heads, seq_size, value_size) 相乘 -> (batch_size*num_heads, seq_size, value_size)\n",
    "\n",
    "        # 组合多头。此时的Z的维度为：(batch_size*num_heads, seq_size, value_size)，我们需要进行一定的转化\n",
    "        attention_outputs_concat = transpose_output(attention_outputs, self.num_heads) # Z_concat的维度：(batch_size, seq_size, num_heads*value_size)\n",
    "\n",
    "        outputs = self.W_o(attention_outputs_concat) # outputs的维度：(batch_size, seq_size, num_hiddens)\n",
    "        # print(outputs, \"####################\\n\")\n",
    "        return outputs\n",
    "\n",
    "    def masked_softmax(self, scores, valid_lens):\n",
    "        # 输入 scores 的维度：(batch_size*num_heads, seq_size, seq_size)\n",
    "        # 输入 valid_lens 的维度：1.(1, batch_size) 行向量；2.(seq_size, seq_size)\n",
    "        dim1, seq_size, _ = scores.shape\n",
    "        if valid_lens is None:\n",
    "            return nn.functional.softmax(scores, dim=-1) \n",
    "        else: \n",
    "            if valid_lens.dim() == 1:\n",
    "                valid_lens = valid_lens.repeat_interleave(self.num_heads, dim=0)\n",
    "                mask = torch.arange(seq_size, device=scores.device)[None, :] < valid_lens[:, None] \n",
    "                scores[~mask] = -100000 \n",
    "                # scores[~mask] = float('-inf')\n",
    "            elif valid_lens.dim() == 2:\n",
    "                valid_lens = valid_lens.bool()  \n",
    "                mask = valid_lens.unsqueeze(0).unsqueeze(0).repeat(int(dim1/self.num_heads), self.num_heads, 1, 1)\n",
    "                print(mask.shape)\n",
    "                mask = mask.view(dim1, seq_size, seq_size)\n",
    "                scores[mask] = -100000\n",
    "                # scores[mask] = float('-inf')\n",
    "\n",
    "            return nn.functional.softmax(scores, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "643b2e87-efe3-4500-b169-fd6faf1863b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# scores = torch.rand(batch_size*num_heads, seq_size, seq_size)\n",
    "\n",
    "# valid_lens = torch.tensor([3,4,5])\n",
    "# valid_lens = valid_lens.repeat_interleave(num_heads, dim=0)\n",
    "# mask = torch.arange(seq_size)[None, :] < valid_lens[:, None] \n",
    "# scores[~mask] = -100000 \n",
    "# print(mask, \"\\n\", scores)\n",
    "\n",
    "# dec_valid_lens = torch.triu(torch.ones(seq_size, seq_size), diagonal=1)  \n",
    "# dec_valid_lens = dec_valid_lens.bool() \n",
    "# mask = dec_valid_lens.unsqueeze(0).unsqueeze(0).repeat(batch_size, num_heads, 1, 1)\n",
    "# mask = mask.view(batch_size*num_heads, seq_size, seq_size)\n",
    "# scores[mask] = -100000\n",
    "# print(mask, \"\\n\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de066853-8b4c-40c4-af8a-d74a35cc5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(normalized_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(num_hiddens, ffn_num_hiddens)\n",
    "        self.addnorm2 = AddNorm(normalized_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        # print(\"encoder的多头注意力\")\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer编码器\"\"\"\n",
    "    def __init__(self, src_vocab_size, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embedding_size = num_hiddens\n",
    "        self.embedding = nn.Embedding(src_vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), EncoderBlock(num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, use_bias))\n",
    "\n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.embedding_size)) \n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens) \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31ea5cd0-9c1b-4451-a9a7-ec84eb19ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中第i个块\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention1 = MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(normalized_shape, dropout)\n",
    "        self.attention2 = MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(normalized_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(num_hiddens, ffn_num_hiddens)\n",
    "        self.addnorm3 = AddNorm(normalized_shape, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "\n",
    "        if state[2][self.i] is None: \n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1) \n",
    "        state[2][self.i] = key_values \n",
    "\n",
    "        # dec_valid_lens的作用是未来掩码，但目前我不清楚是如何运作的\n",
    "        if self.training: \n",
    "            _, seq_size, _ = X.shape\n",
    "            # dec_valid_lens = torch.arange(1, seq_size + 1, device=X.device).repeat(batch_size, 1) \n",
    "            dec_valid_lens = torch.triu(torch.ones(seq_size, seq_size, device=X.device), diagonal=1)\n",
    "        else: \n",
    "            dec_valid_lens = None # 构建dec_valid_lens，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。\n",
    "\n",
    "        # 自注意力\n",
    "        # print(\"decoder的第一个多头注意力\")\n",
    "        X1 = self.attention1(X, key_values, key_values, dec_valid_lens) # 第一次的输入还是X,X,X\n",
    "        X2 = self.addnorm1(X, X1)\n",
    "        # 编码器－解码器注意力。\n",
    "        # print(\"decoder的第二个多头注意力\")\n",
    "        X3 = self.attention2(X2, enc_outputs, enc_outputs, enc_valid_lens) # 这里如何理解？？？？？？\n",
    "        Y = self.addnorm2(X2, X3)\n",
    "        return self.addnorm3(Y, self.ffn(Y)), state\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embedding_size = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(tgt_vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), DecoderBlock(num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, i))\n",
    "        self.dense = nn.Linear(num_hiddens, tgt_vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        state = [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "        return state\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.embedding_size))\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state)\n",
    "        return self.dense(X), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6db6d46-3f9f-4398-b9a1-b3f190d16dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, X_valid_len):\n",
    "        enc_outputs = self.encoder(enc_X, X_valid_len)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, X_valid_len)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0c645ae-cd0a-4a84-a306-c7ee74912336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    \n",
    "    def xavier_init_weights(m): # 使用 Xavier 均匀分布初始化 Transformer 模型中的权重。\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    model.apply(xavier_init_weights)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = MaskedSoftmaxCELoss()\n",
    "    model.train()\n",
    "    visualizer = TrainingVisualizer(xlabel='Epoch', ylabel='Value', title='Train transformer_scratch', legend=['Train Loss'])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for X, X_valid_len, Y, Y_valid_len in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = X.to(device), X_valid_len.to(device), Y.to(device), Y_valid_len.to(device)\n",
    "            # print(X, X_valid_len, Y, Y_valid_len, \"#######################################################\")\n",
    "            \n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1) # Y.shape[0]为batch_size，\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学。dim=0，加行；dim=1，加列。Y[:, :-1]选择所有行，列从第一列到倒数第二列\n",
    "\n",
    "            # X_valid_len = None # #########################################################################################\n",
    "            Y_hat, _ = model(X, dec_input, X_valid_len) # 编码器输入：(X, X_valid_len)；解码器输入：(dec_input, 初始化的state)\n",
    "            # print(Y_hat)\n",
    "            loss = criterion(Y_hat, Y, Y_valid_len)\n",
    "            # print(loss)\n",
    "            total_loss+=loss.sum().item()\n",
    "            loss.sum().backward()\t\n",
    "            grad_clipping(model, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            \n",
    "        visualizer.add(epoch, [total_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba3b425-e7ed-4618-9d0b-f2bcc0438116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n",
      "torch.Size([64, 8, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = TransformerEncoder(src_vocab_size, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout)\n",
    "decoder = TransformerDecoder(tgt_vocab_size, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout)\n",
    "\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "train_seq2seq(model, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab651de-336a-44a5-be2a-04c5cd5e1ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b654b5f-003a-43d6-8fcd-c9228ca647ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af46a2f-2515-4182-89e6-a0008c32bc61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6894e3ed-f045-41bd-afc5-8bd0b6f6e594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2b05b-aa23-4cf0-adae-18aaf51cc4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb90a0c-bc2b-4da3-8c2b-758e7088e378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6cbe6-d5b6-42b3-9c1b-f295ed792098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1852c3b-0e64-4b2e-b0e0-0a25e2d2af59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47373769-0b24-4b09-aac6-857a3a732bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3924dff6-3e27-4a27-b452-61d2269742dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d6b1cf-4001-4d9b-9e18-f3b7a1698c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237ab88-ae77-452f-ada5-73ec9b01242a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cebfcb-c21e-4ad3-9c98-b938fdda5883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9aaa6-8afe-4a95-a328-aac431fc61f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d2fcc-e9df-4814-8202-d50074854017",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aa7ad5-e111-4614-a196-9c74bfa0f7f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c985f18d-a75e-434f-a902-1acd0746ccda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef509459-626a-4105-8ab4-b1abc96c2487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
