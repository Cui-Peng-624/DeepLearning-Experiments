{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30fe75bd-f8ff-4101-b661-b9aa0cbe3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from visualization import TrainingVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50b61eca-c6dc-417e-aab0-adf1afffc187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3731779-a2bd-40e9-af79-693887fa9d5d",
   "metadata": {},
   "source": [
    "# 处理文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115a90a4-f076-414f-ae9f-251766245618",
   "metadata": {},
   "source": [
    "## 1.直接读"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892387e5-1fba-473b-adb8-c03cd591828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文本数据\n",
    "with open('data/time_machine_txt/timemachine.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 创建字符映射表\n",
    "chars = sorted(list(set(text))) # 字母级别的token\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "\n",
    "# 转换文本为索引\n",
    "text_as_int = np.array([char_to_idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c273f3f-b652-4881-905e-f3c63158facf",
   "metadata": {},
   "source": [
    "## 2.考虑大小写和阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04433e08-8f56-4a8b-b81b-42957c890bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置阈值\n",
    "threshold = 5\n",
    "\n",
    "# 加载文本数据\n",
    "with open('data/time_machine_txt/timemachine.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 将所有字符转换为小写\n",
    "text = text.lower()\n",
    "\n",
    "# 计算字符的词频\n",
    "char_counts = Counter(text)\n",
    "\n",
    "# 创建字符映射表，保留词频不低于阈值的字符，其他字符设为<unk>\n",
    "chars = sorted([ch for ch, count in char_counts.items() if count >= threshold])\n",
    "chars.append('<unk>')  # 添加<unk>标记\n",
    "\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "unk_idx = char_to_idx['<unk>']\n",
    "\n",
    "# 转换文本为索引，如果字符词频低于阈值，则转换为<unk>\n",
    "text_as_int = np.array([char_to_idx.get(c, unk_idx) for c in text])\n",
    "\n",
    "# 打印结果示例\n",
    "print(\"Unique characters:\", len(chars))\n",
    "print(\"Character to Index mapping:\", char_to_idx)\n",
    "print(\"First 100 characters as indices:\", text_as_int[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578eb1b7-c606-49fd-9cdf-b7444477e546",
   "metadata": {},
   "source": [
    "## 3.考虑大小写和阈值，不考虑标点符号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0023fb2d-03ab-4c1a-aa26-27086d678e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: 30\n",
      "Character to Index mapping: {'\\n': 0, ' ': 1, '_': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, '<unk>': 29}\n",
      "First 100 characters as indices: [22 10  7  1 22 11 15  7  1 15]\n"
     ]
    }
   ],
   "source": [
    "# 设置阈值\n",
    "threshold = 5\n",
    "\n",
    "# 加载文本数据\n",
    "with open('data/time_machine_txt/timemachine.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 将所有字符转换为小写\n",
    "text = text.lower()\n",
    "\n",
    "# 移除所有标点符号\n",
    "text = re.sub(r'[^\\w\\s]', '', text)  # 仅保留字母、数字和空格\n",
    "\n",
    "# 计算字符的词频\n",
    "char_counts = Counter(text)\n",
    "\n",
    "# 创建字符映射表，保留词频不低于阈值的字符，其他字符设为<unk>\n",
    "chars = sorted([ch for ch, count in char_counts.items() if count >= threshold])\n",
    "chars.append('<unk>')  # 添加<unk>标记\n",
    "\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "unk_idx = char_to_idx['<unk>']\n",
    "\n",
    "# 转换文本为索引，如果字符词频低于阈值，则转换为<unk>\n",
    "text_as_int = np.array([char_to_idx.get(c, unk_idx) for c in text])\n",
    "\n",
    "# 打印结果示例\n",
    "print(\"Unique characters:\", len(chars))\n",
    "print(\"Character to Index mapping:\", char_to_idx)\n",
    "print(\"First 100 characters as indices:\", text_as_int[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5badf602-b33e-4bd3-835d-0511e03c7ccb",
   "metadata": {},
   "source": [
    "# 设置统一的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46c5579-11b8-437a-a25e-0b733525f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "vocab_size = len(chars)  # 字符的个数\n",
    "seq_size = 50  # 序列长度 - 一个句子100个单词\n",
    "batch_size = 128\n",
    "hidden_size = 256\n",
    "embedding_size = 64  # input_size 嵌入向量的大小\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8177055-9676-4f1a-b448-767efabfe375",
   "metadata": {},
   "source": [
    "# 转化成适合训练的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f544420b-9661-4664-9c83-ecd134b202c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(text_as_int, seq_size, batch_size):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(0, len(text_as_int) - seq_size):\n",
    "        sequences.append(text_as_int[i:i + seq_size])\n",
    "        targets.append(text_as_int[i + 1:i + seq_size + 1])\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    dataset = torch.utils.data.TensorDataset(sequences, targets)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataset(text_as_int, seq_size, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4b1f30e-82e6-4263-bc5c-93286dd9a8c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1355\n",
      "Batch 101:\n",
      "Data:\n",
      "torch.Size([128, 50])\n",
      "Labels:\n",
      "torch.Size([128, 50])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader))\n",
    "# test - 查看 DataLoader 中的具体元素\n",
    "for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "    if batch_idx >= 100:\n",
    "        print(f\"Batch {batch_idx + 1}:\")\n",
    "        print(\"Data:\")\n",
    "        print(data.shape)\n",
    "        print(\"Labels:\")\n",
    "        print(labels.shape)\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87e6e1-1b1b-47b5-aec5-1cc650777151",
   "metadata": {},
   "source": [
    "# 从矩阵相乘开始重构RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "743899e4-f29e-4f89-891f-ec1e6cc44960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_Wb(embedding_size, hidden_size, vocab_size, device):\n",
    "    \"\"\"\n",
    "    hidden_size：神经元个数\n",
    "    注意：虽然一般的X为(batch_size, seq_size, vocab_size)，但是由于RNN的特性，每次循环是对于一个时间步，所以X的size一般需要转化成(seq_size, batch_size, vocab_size)，\n",
    "    那么每次循环我们需要处理的输入的size就是：(batch_size, vocab_size)；此外，由于我们需要经过一个嵌入层，所以我们实际需要处理的size为：(batch_size, embedding_size)\n",
    "    \"\"\"\n",
    "    # 隐藏层参数\n",
    "    # X's size: (batch_size, seq_size, vocab_size) -> (seq_size, batch_size, vocab_size) -> (seq_size, batch_size, embedding_size)\n",
    "    W_xh =  torch.normal(0, 1, (embedding_size, hidden_size), device = device) * 0.01\n",
    "    # H = torch.normal(0, 1, (batch_size, hidden_size))\n",
    "    W_hh = torch.normal(0, 1, (hidden_size, hidden_size), device = device) * 0.01\n",
    "    b_h = torch.zeros((1, hidden_size), device = device) # 注意这里的偏置项是加在神经元上，不是batch_size*hidden_size\n",
    "\n",
    "    # 输出层参数\n",
    "    W_hq = torch.normal(0, 1, (hidden_size, vocab_size), device = device) * 0.01\n",
    "    b_q = torch.zeros((1, vocab_size), device = device)\n",
    "    # 然后经过softmax，得到每个vocab的输出概率？？？\n",
    "\n",
    "    # 附加梯度\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea855623-da4d-49ef-93bd-f08628ef53fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数初始化隐变量H\n",
    "def initialize_H(batch_size, hidden_size, device):\n",
    "    H = torch.zeros((batch_size, hidden_size), device = device)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86bf0706-51fd-4e97-862f-c7ecf5ead0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义矩阵计算\n",
    "def RNN_calculate(inputs, params, H, device):\n",
    "    outputs = []\n",
    "    # inputs的形状：(seq_size，batch_size, embedding_size)\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    for X in inputs: # X的形状：(batch_size, embedding_size)\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        O = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(O) # outputs是每一个时间步的输出 - [tensor(batch_size, vocab_size), tensor(batch_size, vocab_size), ......]\n",
    "        \n",
    "    outputs = torch.stack(outputs)\n",
    "    return outputs, H # outputs 的 size：(seq_size, batch_size, vocab_size)；H 的 size：(batch_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07f292aa-6a4e-40b3-ae88-6b83c2425471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 128, 30]), torch.Size([128, 256]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "inputs = torch.normal(0, 1, (seq_size, batch_size, embedding_size), device = device)\n",
    "\n",
    "params = initialize_Wb(embedding_size, hidden_size, vocab_size, device)\n",
    "H = initialize_H(batch_size, hidden_size, device)\n",
    "\n",
    "outputs, H = RNN_calculate(inputs, params, H, device)\n",
    "\n",
    "outputs.shape, H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b5137b-71b3-4efe-8cce-4575acc78357",
   "metadata": {},
   "source": [
    "# 定义RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0edd875-7383-45a9-978d-2aa1342d4aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, vocab_size, initialize_Wb, initialize_H, RNN_calculate, device):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.params = initialize_Wb(embedding_size, hidden_size, vocab_size, device)\n",
    "        self.initialize_H = initialize_H\n",
    "        self.RNN_calculate = RNN_calculate\n",
    "\n",
    "    def forward(self, X, H): \n",
    "        # 输入的X的形状为：(batch_size, seq_size)；H的形状为：(batch_size, hidden_size)\n",
    "        X = self.embedding(X) # 此时X的形状变为：(batch_size, seq_size, embedding_size)\n",
    "        X = X.permute(1, 0, 2).to(self.device) # 此时X的形状变为：(seq_size, batch_size, embedding_size)\n",
    "        Y, H = self.RNN_calculate(X, self.params, H, self.device)\n",
    "        return Y, H # 返回Y的size：(seq_size, batch_size, vocab_size)\n",
    "\n",
    "    def begin_H(self, batch_size, hidden_size, device):\n",
    "        return self.initialize_H(batch_size, hidden_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "04985d46-751b-4d3a-ae17-1d45fb75c5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 128, 30]), torch.Size([128, 256]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "X = torch.randint(0, vocab_size, (batch_size, seq_size))\n",
    "\n",
    "net = RNNModel(embedding_size, hidden_size, vocab_size, initialize_Wb, initialize_H, RNN_calculate, device)\n",
    "\n",
    "H =  net.begin_H(batch_size, hidden_size, device)\n",
    "\n",
    "Y, H = net(X, H)\n",
    "\n",
    "Y.shape, H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a801f91-e82e-48c0-8730-39b1225d7298",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f4263a0c-729e-4712-9105-7a45c71addc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '_': 2, 'a': 3, 'b': 4, 'c': 5, 'd': 6, 'e': 7, 'f': 8, 'g': 9, 'h': 10, 'i': 11, 'j': 12, 'k': 13, 'l': 14, 'm': 15, 'n': 16, 'o': 17, 'p': 18, 'q': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26, 'y': 27, 'z': 28, '<unk>': 29} \n",
      " {0: '\\n', 1: ' ', 2: '_', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z', 29: '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_idx, \"\\n\", idx_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0b728269-b076-4072-9275-15b1f60369ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prediction(prefix, num_preds, net, device):\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    H =  net.begin_H(1, hidden_size, device) # def begin_H(self, batch_size, hidden_size, device):\n",
    "    outputs = [char_to_idx[prefix[0]]] # 初始化为第一个字符的数字表示\n",
    "    \n",
    "    # 定义一个匿名函数（lambda 函数）。从变量 outputs 的最后一个元素创建一个新的张量，并将其形状调整为 (1, 1)。\n",
    "    get_input = lambda: torch.tensor([outputs[-1]]).reshape((1, 1))\n",
    "    \n",
    "    for char in prefix[1:]:  # 预热期 - 逐渐把outputs中的元素添加进model，更新H - 看看H的计算公式，理解是如何更新H的\n",
    "        _, H = net(get_input(), H)\n",
    "        outputs.append(char_to_idx[char])\n",
    "    # 到这里，outputs为prefix每个字母转化为它的数字表示的一维list\n",
    "     \n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        Y, H = net(get_input(), H) # # 返回Y的size：(seq_size, batch_size, vocab_size)\n",
    "        # print(Y.argmax(dim=2).reshape(1))\n",
    "        outputs.append(int(Y.argmax(dim=2).reshape(1)))\n",
    "    return ''.join([idx_to_char[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "56d3340c-fedf-4e64-b14b-ff97c6443498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller <unk>gba\\nnh<unk>e<unk>'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "text_prediction('time traveller ', 10, net, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311b3a20-a54c-4a57-9d98-62499d5237c0",
   "metadata": {},
   "source": [
    "# 梯度裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6b27e5e-496a-4952-8caa-4b0b890707ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm # 整体进行一个缩放"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4ccc3-3059-4a09-9f92-2ec257467b61",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "da8ee25f-3648-4d67-afcc-158fe43515d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1:\n",
      "Data:\n",
      "torch.Size([128, 50])\n",
      "Labels:\n",
      "torch.Size([128, 50])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test - 查看 DataLoader 中的具体元素\n",
    "for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}:\")\n",
    "    print(\"Data:\")\n",
    "    print(data.shape)\n",
    "    print(\"Labels:\")\n",
    "    print(labels.shape)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9278c53-6dbf-4732-912c-0ed688be8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_train_epoch(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期\"\"\"\n",
    "    H = None\n",
    "    metric = Accumulator(2)  # 训练损失之和,词元数量 - 作用暂时未知\n",
    "    for X, Y in train_iter:\n",
    "        # print(X.shape, Y.shape) # torch.Size([128, 50]) torch.Size([128, 50])\n",
    "        # print(type(H)) # - <class 'NoneType'> -> <class 'torch.Tensor'>\n",
    "\n",
    "        if H is None or use_random_iter or H.size(0) != X.size(0):\n",
    "            H = net.begin_H(X.shape[0], hidden_size, device)\n",
    "        # if H is None or use_random_iter:\n",
    "        #     # 在第一次迭代或使用随机抽样时初始化state\n",
    "        #     H = net.begin_H(X.shape[0], hidden_size, device) # batch_size 等于 X 的第一个维度（从一开始）\n",
    "        else: # 如果不是第一次迭代，并且没有使用随机抽样，需要维护 state 的梯度信息\n",
    "            if isinstance(net, nn.Module) and not isinstance(H, tuple):  \n",
    "                # state对于nn.GRU是个张量 - 如果 net 是 nn.Module 的实例，并且 state 不是元组（即 state 是一个张量，通常用于 nn.GRU），则直接对 state 调用 detach_()\n",
    "                H.detach_()\n",
    "            else: \n",
    "                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量\n",
    "                # 如果 state 是一个元组（通常用于 nn.LSTM 或自定义的 RNN 实现），则需要对元组中的每个张量调用 detach_()。\n",
    "                for h in H:\n",
    "                    h.detach_()\n",
    "\n",
    "        y = Y.T # Y的size从(batch_size, seq_size) -> (seq_size, batch_size)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, H = net(X, H)\n",
    "        # 为了计算交叉熵损失，我们需要调整y_hat和y的size，这也是课本代码中使用reshape的原因\n",
    "        # print(y_hat.shape, y.shape)\n",
    "        y_hat = y_hat.reshape(-1, y_hat.shape[2])\n",
    "        y = y.reshape(-1) # 把y变为一维的张量\n",
    "        # print(y_hat.shape, y.shape)\n",
    "        \n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        # print(l)\n",
    "        \n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了mean函数\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "        \n",
    "    return math.exp(metric[0] / metric[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566f7ec-ab9f-43fa-b22c-f442c4f245ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(params, lr, batch_size):  \n",
    "    \"\"\"小批量随机梯度下降\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param -= lr * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "\n",
    "def RNN_train(net, train_iter, char_to_idx, lr, num_epochs, device, use_random_iter=False):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    visualizer = TrainingVisualizer(xlabel='Epoch', ylabel='Value', title='Train RNN_scratch in Time_Machine', legend=['ppl'])  \n",
    "\n",
    "    # 初始化 - 为啥要这样处理，分为是不是nn.Module? - 不是nn.Module当然没法用nn库里的梯度下降方法 maybe\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: sgd(net.params, lr, batch_size)\n",
    "        \n",
    "    predict = lambda prefix: text_prediction(prefix, 50, net, char_to_idx, device) # 匿名函数，输入prefix预测50个token\n",
    "    \n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl = RNN_train_epoch(net, train_iter, loss, updater, device, use_random_iter)\n",
    "        \n",
    "        # if (epoch + 1) % 10 == 0:\n",
    "        #     print(predict('time traveller'))\n",
    "        #     # 更新可视化\n",
    "        #     visualizer.add(epoch, [ppl])\n",
    "        visualizer.add(epoch, [ppl])\n",
    "             \n",
    "    # print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d7252f-431e-4073-aa86-5cbc1a7c2ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "RNN_train(net, dataloader, char_to_idx, lr, num_epochs, device)\n",
    "\n",
    "# 出现了最后一个batch的大小小于设置的batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897d280-4870-47e8-9701-182c12f32ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b5daa-0426-42c2-a103-867dadc92e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed080a4-1c38-4b4b-b460-483d4810a1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54589dcb-7bef-470b-9fbe-9265f6085b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e77a7-a6f3-4905-852a-8965840e4cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b80c4-72cc-4127-b970-de7624cc6ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f4a4c-ce93-47da-80de-4d85bf90e072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e42ce-95d8-46ca-9947-123397dd5992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1d3a464-7863-44b4-bb71-374b21d71138",
   "metadata": {},
   "source": [
    "将 `__call__` 方法改成 `forward` 方法有多个好处，特别是在使用 PyTorch 框架进行深度学习模型开发时：\r\n",
    "\r\n",
    "### PyTorch 的设计约定\r\n",
    "\r\n",
    "1. **符合 PyTorch 的约定**：\r\n",
    "   - 在 PyTorch 中，所有继承自 `nn.Module` 的类都应定义 `forward` 方法，而不是 `__call__` 方法。\r\n",
    "   - 当调用模型实例时，例如 `output = model(input)`，PyTorch 会自动调用 `forward` 方法。这是 PyTorch 的设计约定，确保代码的可读性和一致性。\r\n",
    "\r\n",
    "2. **支持内置功能**：\r\n",
    "   - 使用 `forward` 方法可以使模型更好地与 PyTorch 的许多内置功能和模块集成，如 `torch.jit.trace`、`torch.nn.DataParallel` 等。\r\n",
    "   - 这些功能通常会依赖于 `forward` 方法的存在，而不是 `__call__` 方法。\r\n",
    "\r\n",
    "### 代码示例\r\n",
    "\r\n",
    "以下是你的修改后的 `RNNModel` 类，使用 `forward` 方法：\r\n",
    "\r\n",
    "```python\r\n",
    "class RNNModel(nn.Module):\r\n",
    "    def __init__(self, vocab_size, hidden_size, seq_size, embedding_size, device, \r\n",
    "                 initialize_Wb, initialize_H, RNN_calculate):\r\n",
    "        super(RNNModel, self).__init__()  # 调用 nn.Module 的构造函数\r\n",
    "        self.vocab_size = vocab_size\r\n",
    "        self.hidden_size = hidden_size\r\n",
    "        self.seq_size = seq_size\r\n",
    "        self.embedding_size = embedding_size\r\n",
    "        self.device = device\r\n",
    "        \r\n",
    "        self.params = initialize_Wb(vocab_size, hidden_size, embedding_size, device)\r\n",
    "        self.initial_H = initialize_H\r\n",
    "        self.RNN_calculate = RNN_calculate\r\n",
    "\r\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size).to(device)\r\n",
    "\r\n",
    "    def forward(self, X, H):  # 将 __call__ 方法改为 forward\r\n",
    "        X = X.to(self.device)  # 确保输入在正确的设备上\r\n",
    "        X = self.embedding(X).type(torch.float32)\r\n",
    "        Y, H = self.RNN_calculate(X, self.params, H, self.device)\r\n",
    "        return Y, H\r\n",
    "\r\n",
    "    def begin_H(self, seq_size, hidden_size, device):\r\n",
    "        return self.initial_H(seq_size, hidden_size, device)\r\n",
    "```\r\n",
    "\r\n",
    "### 调用模型\r\n",
    "\r\n",
    "当定义了 `forward` 方法后，可以像调用函数一样调用模型实例，PyTorch 会自动调用 `forward` 方法：\r\n",
    "\r\n",
    "```python\r\n",
    "# 初始化模型\r\n",
    "model = RNNModel(vocab_size, hidden_size, seq_size, embedding_size, device, \r\n",
    "                 initialize_Wb, initialize_H, RNN_calculate)\r\n",
    "\r\n",
    "# 示例输入\r\n",
    "X = torch.randint(0, vocab_size, (batch_size, seq_size), dtype=torch.long).to(device)\r\n",
    "H = model.begin_H(batch_size, hidden_size, device)\r\n",
    "\r\n",
    "# 调用模型\r\n",
    "output, H = model(X, H)\r\n",
    "```\r\n",
    "\r\n",
    "### 主要好处总结\r\n",
    "\r\n",
    "1. **代码一致性和可读性**：\r\n",
    "   - 遵循 PyTorch 的设计约定，保持代码风格一致，使其他开发者更容易理解和维护代码。\r\n",
    "\r\n",
    "2. **支持 PyTorch 的高级功能**：\r\n",
    "   - 使用 `forward` 方法可以与 PyTorch 的许多高级功能和模块更好地集成，如模型的序列化、并行计算和优化等。\r\n",
    "\r\n",
    "3. **简化调用**：\r\n",
    "   - 通过定义 `forward` 方法，可以直接调用模型实例进行前向传播，而不需要额外定义 `__call__` 方法。\r\n",
    "\r\n",
    "### 总结\r\n",
    "\r\n",
    "使用 `forward` 方法是 PyTorch 中定义模型前向传播的标准方式。它不仅符合 PyTorch\n",
    "\n",
    "    # def __call__(self, X, H): # state 就是 H\n",
    "    #     # print(\"进入嵌入层之前的X的维度为\", X.shape)\n",
    "    #     X = self.embedding(X).type(torch.float32)\n",
    "    #     # print(\"经过嵌入层之后的X的维度为\", X.shape)\n",
    "    #     return self.RNN_calculate(X, self.params, H, self.device) 的设计约定，提高代码的一致性和可读性，还能更好地支持 PyTorch 的许多内置功能和工具。通过遵循这些约定，可以使你的代码更加规范和易于维护。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5ab35-f87f-44ce-9082-821f7702078f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66b28c7-d026-4f1d-9e92-20adac83b8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fcb7d-26a7-4ff7-a801-50d36cb0a60a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
