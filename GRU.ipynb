{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ebef6b-d211-4f68-8f2d-ee7b0513dc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from visualization import TrainingVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7138eb27-852f-4645-87c6-74f9466acd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization.py\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import MaxNLocator\n",
    "# from IPython import display\n",
    "\n",
    "# class TrainingVisualizer:\n",
    "#     \"\"\"在动画中绘制数据并记录最高的acc和最低的loss\"\"\"\n",
    "#     def __init__(self, xlabel=None, ylabel=None, title=None, legend=None, xlim=None,\n",
    "#                  ylim=None, xscale='linear', yscale='linear',\n",
    "#                  fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "#                  figsize=(7, 5)):\n",
    "#         if legend is None:\n",
    "#             legend = []\n",
    "#         self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "#         if nrows * ncols == 1:\n",
    "#             self.axes = [self.axes, ]\n",
    "#         self.config_axes = lambda: self.set_axes(\n",
    "#             self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
    "#         self.X, self.Y, self.fmts = None, None, fmts\n",
    "#         self.legend = legend\n",
    "#         self.title = title  # 保存标题\n",
    "\n",
    "#         # 新增：记录最高acc和最低loss\n",
    "#         self.best_acc = float('-inf')  # 初始化为负无穷\n",
    "#         self.min_loss = float('inf')   # 初始化为正无穷\n",
    "\n",
    "#     def set_axes(self, ax, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
    "#         \"\"\"设置matplotlib的轴\"\"\"\n",
    "#         ax.set_xlabel(xlabel)\n",
    "#         ax.set_ylabel(ylabel)\n",
    "#         ax.set_xscale(xscale)\n",
    "#         ax.set_yscale(yscale)\n",
    "#         if xlim:\n",
    "#             ax.set_xlim(xlim)\n",
    "#         if ylim:\n",
    "#             ax.set_ylim(ylim)\n",
    "#         if legend:\n",
    "#             ax.legend(legend)\n",
    "#         ax.xaxis.set_major_locator(MaxNLocator(integer=True))  # 设置x轴刻度为整数\n",
    "#         ax.grid()\n",
    "        \n",
    "#     def add(self, x, y):\n",
    "#         if not hasattr(y, \"__len__\"):\n",
    "#             y = [y]\n",
    "#         n = len(y)\n",
    "#         if not hasattr(x, \"__len__\"):\n",
    "#             x = [x] * n\n",
    "#         if self.X is None:\n",
    "#             self.X = [[] for _ in range(n)]\n",
    "#         if self.Y is None:\n",
    "#             self.Y = [[] for _ in range(n)]\n",
    "#         for i, (a, b) in enumerate(zip(x, y)):\n",
    "#             if a is not None and b is not None:\n",
    "#                 self.X[i].append(a)\n",
    "#                 self.Y[i].append(b)\n",
    "\n",
    "#                 # 更新最高acc和最低loss\n",
    "#                 if self.legend[i].lower().find('acc') != -1:  # 检查是否是acc数据\n",
    "#                     if b > self.best_acc:\n",
    "#                         self.best_acc = b\n",
    "#                 if self.legend[i].lower().find('loss') != -1:  # 检查是否是loss数据\n",
    "#                     if b < self.min_loss:\n",
    "#                         self.min_loss = b\n",
    "\n",
    "#         self.axes[0].cla()\n",
    "#         for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "#             self.axes[0].plot(x, y, fmt)\n",
    "#         self.config_axes()\n",
    "\n",
    "#         # 显示最高acc和最低loss\n",
    "#         if self.title:\n",
    "#             self.axes[0].set_title(self.title + f\"\\nBest Acc: {self.best_acc:.4f}, Min Loss: {self.min_loss:.4f}\")\n",
    "#         else:\n",
    "#             self.axes[0].set_title(f\"Best Acc: {self.best_acc:.4f}, Min Loss: {self.min_loss:.4f}\")\n",
    "        \n",
    "#         display.display(self.fig)\n",
    "#         display.clear_output(wait=True)\n",
    "\n",
    "# # 调用示例\n",
    "# # visualizer = TrainingVisualizer(xlabel='Epoch', ylabel='Value', title='Train TextRCNN', legend=['training_loss', \"training_acc\", \"testing_loss\", \"testing_acc\"]) # 初始化\n",
    "# # for epoch in range(num_epochs):\n",
    "# #     train_acc, train_loss = \n",
    "# #     val_acc, val_loss = \n",
    "# #     visualizer.add(epoch, [train_loss, train_acc, val_loss, val_acc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2553a602-ec00-41cd-88b5-fd2f4178fd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a06cf61-3813-4244-985a-9119f75ac947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载文本数据\n",
    "with open('data/time_machine_txt/timemachine.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# 创建字符映射表\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "idx_to_char = {idx: ch for idx, ch in enumerate(chars)}\n",
    "\n",
    "# 转换文本为索引\n",
    "text_as_int = np.array([char_to_idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d95358-4b14-423f-b13d-50e95e947cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20703\\AppData\\Local\\Temp\\ipykernel_12996\\3710666413.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  sequences = torch.tensor(sequences, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# 定义超参数\n",
    "seq_length = 100  # 序列长度 - 一个句子100个单词\n",
    "batch_size = 256\n",
    "hidden_size = 256\n",
    "embedding_dim = 64  # input_size 嵌入向量的大小\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "vocab_size = len(chars)  # 字符的个数\n",
    "\n",
    "def create_dataset(text_as_int, seq_length, batch_size):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    for i in range(0, len(text_as_int) - seq_length):\n",
    "        sequences.append(text_as_int[i:i + seq_length])\n",
    "        targets.append(text_as_int[i + 1:i + seq_length + 1])\n",
    "    sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    dataset = torch.utils.data.TensorDataset(sequences, targets)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "dataloader = create_dataset(text_as_int, seq_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ea267b-f29d-4365-af2a-2a828b50f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义GRU模型\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = out.contiguous().view(-1, hidden.size(2))\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.randn(num_layers, batch_size, hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fadebc-8bc0-4861-a186-ff3c672f769e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _draw_all_if_interactive at 0x0000027970C4EB80> (for post_execute):\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# 实例化模型\n",
    "model = GRUModel(vocab_size, embedding_dim, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "visualizer = TrainingVisualizer(xlabel='Epoch', ylabel='Value', title='Train GRU in Time_Machine', legend=['Train Loss', 'Perplexity'])\n",
    "\n",
    "# 训练模型并计算困惑度\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        current_batch_size = inputs.size(0)\n",
    "        inputs = inputs.view(current_batch_size, -1).to(device)\n",
    "        targets = targets.view(-1).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden(current_batch_size).detach()\n",
    "        output, hidden = model(inputs, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}')\n",
    "\n",
    "    visualizer.add(epoch, [avg_loss, perplexity.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ab70d-0524-4e99-b892-cf12513dbdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成文本\n",
    "def generate_text(model, start_str, length):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    input = torch.tensor([char_to_idx[ch] for ch in start_str], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    generated_text = start_str\n",
    "    for _ in range(length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        _, top_idx = torch.topk(output[-1], 1)\n",
    "        next_char = idx_to_char[top_idx.item()]\n",
    "        generated_text += next_char\n",
    "        input = torch.tensor([[top_idx]], dtype=torch.long).to(device)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# 生成文本示例\n",
    "start_str = \"Filby became pensive. \"\n",
    "generated_text = generate_text(model, start_str, 100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82670bf7-ab05-4d23-8a20-fe3b2cd2db85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
