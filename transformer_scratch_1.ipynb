{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fee477e-8c2f-4b04-8104-f66963f5efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "339f5281-2e83-4b5a-a457-6193f56c7ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d97a0-fcc7-457e-84e0-160051920db4",
   "metadata": {},
   "source": [
    "# 位置编码 - position encoding\n",
    "$PE(pos,2i) = sin(\\dfrac{pos}{10000^{\\dfrac{2i}{d}}})$，$PE(pos,2i+1) = cos(\\dfrac{pos}{10000^{\\dfrac{2i}{d}}})$        \n",
    "pos表示token在句子中的位置，d代表词嵌入的维度，2i代表在词嵌入维度中的第几维  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa89fcd-1747-4710-b8d7-0e71422c8c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "max_len = 1000\n",
    "numerator = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) # 定义分子 - 转变为(max_len, 1)\n",
    "denominator = torch.pow(10000, torch.arange(0, embedding_size, 2, dtype=torch.float32) / embedding_size) # 定义分母 - 输出维度为(embedding_size/2, 1)\n",
    "fraction = numerator / denominator\n",
    "numerator.shape, denominator.shape, fraction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13dd1df-e6fb-4ffd-be58-e6f7722449f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_size, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 创建一个足够长的P\n",
    "        self.P = torch.zeros((1, max_len, embedding_size)) \n",
    "        \n",
    "        numerator = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) # 定义分子 - 转变为(max_len, 1)\n",
    "        denominator = torch.pow(10000, torch.arange(0, embedding_size, 2, dtype=torch.float32) / embedding_size) # 定义分母 - 输出维度为(1, embedding_size/2)\n",
    "        fraction = numerator / denominator # 输出维度(max_len, embedding_size/2)\n",
    "        \n",
    "        self.P[:, :, 0::2] = torch.sin(fraction) # 0::2 表示从索引 0 开始，每隔两个元素选择一个元素\n",
    "        self.P[:, :, 1::2] = torch.cos(fraction) # 1::2 表示从索引 1 开始，每隔两个元素选择一个元素。\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 输入维度为：(1, seq_size, embedding_size)\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        X = self.dropout(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528eea75-def7-4c51-a31e-763bd157300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = 50\n",
    "embedding_size = 512\n",
    "dropout = 0.5\n",
    "\n",
    "PE = PositionalEncoding(embedding_size, dropout)\n",
    "X = torch.rand(1, seq_size, embedding_size)\n",
    "X = PE(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7a1b6-9164-4492-bf8b-27bcdc671d35",
   "metadata": {},
   "source": [
    "# 自注意力 - 还没解决掩码问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2330ea-0a59-478b-b654-f2dbe2a5bc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size, seq_size, embedding_size)\n",
    "    # 输出X的形状:(batch_size，seq_size，num_heads，embedding_size/num_heads) 四维\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "\n",
    "    # 输出X的形状:(batch_size，num_heads，查询或者“键－值”对的个数, num_hiddens/num_heads)\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "\n",
    "    # 最终输出的形状:(batch_size*num_heads,查询或者“键－值”对的个数, num_hiddens/num_heads)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a6e11e-e1b7-4239-a96b-2b76f2d4f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_size = 2\n",
    "key_size = 3\n",
    "keys = torch.rand(batch_size, seq_size, key_size)\n",
    "keys.transpose(1, 2).shape, keys, keys.transpose(1, 2), torch.tensor([1]).shape, keys.transpose(1, 2)/torch.tensor([2]), math.sqrt(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79838ee-b389-43be-b422-1c8b3124f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size，seq_size，#_size*num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1) # (batch_size，seq_size，num_heads，#_size) \n",
    "    X = X.permute(0, 2, 1, 3) # (batch_size，num_heads，seq_size, #_size)\n",
    "    X = X.reshape(-1, X.shape[2], X.shape[3]) # 最终输出的形状: (batch_size*num_heads, seq_size, #_size)\n",
    "    return X\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    # 输出维度：(batch_size*num_heads, seq_size, value_size)\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2]) # (batch_size, num_heads, seq_size, value_size)\n",
    "    X = X.permute(0, 2, 1, 3) # (batch_size, seq_size, num_heads, value_size)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], -1) # (batch_size, seq_size, num_heads*value_size)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d51390-b5b7-4676-bf14-8cdf9fabc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # 由于encoder和decoder都包含了多头注意力，所以我们需要考虑掩码Masked的情况\n",
    "    def __init__(self, embedding_size, query_size, key_size, value_size, output_size, num_heads, dropout, bias=False, **kwargs): \n",
    "        # 参数我们一般选取 $p_q h = p_k h = p_v h = p_o$，也就是说 query_size*num_heads = key_size*num_heads = value_size*num_heads = output_size\n",
    "        # 由于 Add & Norm 需要“self.dropout(Y) + X”，所以我们在  Add & Norm 中输入的 X和Y 的维度要匹配，Y是经过多头注意力之后的输出，也就是说 output_size = embedding_size\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.key_size = key_size\n",
    "        self.num_heads = num_heads\n",
    "        self.W_q = nn.Linear(embedding_size, query_size*num_heads, bias=bias)\n",
    "        self.W_k = nn.Linear(embedding_size, key_size*num_heads, bias=bias)\n",
    "        self.W_v = nn.Linear(embedding_size, value_size*num_heads, bias=bias)\n",
    "        self.W_o = nn.Linear(value_size*num_heads, output_size, bias=bias)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        # 输入X的size：(batch_size, seq_size, embedding_size)\n",
    "        # valid_lens的形状:(batch_size，)或者(batch_size，num_hiddens)\n",
    "\n",
    "        queries = self.W_q(X) # (batch_size, seq_size, query_size*num_heads)\n",
    "        keys = self.W_k(X) # (batch_size, seq_size, key_size*num_heads)\n",
    "        values = self.W_v(X) # (batch_size, seq_size, value_size*num_heads)\n",
    "\n",
    "        # 根据我们平板上的推导，我们一开始的想法是错误的\n",
    "        # 在Q K.T之前我们需要对 QKV 进行处理，使得第三个维度 #_size*num_heads 中的 num_heads 到第一个维度 batch_size上去，不要影响 Z_i 的结果\n",
    "        queries = transpose_qkv(queries, self.num_heads) # (batch_size*num_heads, seq_size, query_size)\n",
    "        keys = transpose_qkv(keys, self.num_heads) # (batch_size*num_heads, seq_size, key_size)\n",
    "        values = transpose_qkv(values, self.num_heads) # (batch_size*num_heads, seq_size, value_size)\n",
    "\n",
    "        # 为了计算 Q K^T 我们需要先reshape keys\n",
    "        keys = keys.transpose(1, 2) # 交换keys的第一个和第二个维度（从0开始） -> (batch_size*num_heads, key_size, seq_size)\n",
    "        scores = torch.bmm(queries, keys) # 1.输出维度：(batch_size*num_heads, seq_size, seq_size) 2.我们需要保证 query_size = key_size！\n",
    "        scores = scores / torch.tensor([math.sqrt(self.key_size)]) # 除以 根号下(key_size)\n",
    "        scores = nn.functional.softmax(scores) # (batch_size*num_heads, seq_size, seq_size)\n",
    "        Z = torch.bmm(scores, values) # (batch_size*num_heads, seq_size, seq_size) 与 (batch_size*num_heads, seq_size, value_size) 相乘 -> (batch_size*num_heads, seq_size, value_size)\n",
    "\n",
    "        # 此时的Z的维度为：(batch_size*num_heads, seq_size, value_size)，我们需要进行一定的转化\n",
    "        Z_concat = transpose_output(Z, self.num_heads) # Z_concat的维度：(batch_size, seq_size, num_heads*value_size)\n",
    "        \n",
    "        if valid_lens is not None:\n",
    "            # 在轴0（按行），将第行复制num_heads次。\n",
    "            # 例子：x = torch.tensor([[1, 2], [3, 4]]) result = torch.repeat_interleave(x, repeats=2, dim=0) print(result) 输出：tensor([[1, 2],[1, 2],[3, 4],[3, 4]])\n",
    "            valid_lens = torch.repeat_interleave(valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        outputs = self.W_o(Z_concat) # outputs的维度：(batch_size, seq_size, output_size)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b328d-da6e-463a-b62b-98afb4c6c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "query_size = 32\n",
    "key_size = 32\n",
    "value_size = 32\n",
    "output_size = 256\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "seq_size = 50\n",
    "\n",
    "MHA = MultiHeadAttention(embedding_size, query_size, key_size, value_size, output_size, num_heads, dropout)\n",
    "X = torch.rand(2, seq_size, embedding_size)\n",
    "outputs = MHA(X, None)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbc12a7-5afb-4ccb-a8da-c69e0ce3547f",
   "metadata": {},
   "source": [
    "# Add & Norm - 不改变输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596c71fc-26f4-457b-87b3-515488c3759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        # 输入维度：(batch_size, seq_size, output_size)\n",
    "        # normalized_shape是最后一个维度的大小\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        fx_add_x = self.dropout(Y) + X # 残差连接\n",
    "        outputs = self.ln(fx_add_x) # Layer Normalization层归一化 - 对每个样本的所有特征进行归一化\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5652478-9c9c-4a66-a6ee-7e51b51eb9fd",
   "metadata": {},
   "source": [
    "# Feed Forward 逐位前反馈神经网络 - （Position-wise Feed-Forward Network, 简称 FFN）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fcd5b-53a4-4f79-9d7a-62b6e3333620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs):\n",
    "        # 7.4 晚上 21：01：18 来看ffn_num_input就等于 “class MultiHeadAttention(nn.Module)” 中的 output_size\n",
    "        # 7.4 晚上 21：01：18 来看ffn_num_outputs就等于 embedding_size\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.dense2(X)\n",
    "        return X # (batch_size, seq_size, output_size) -> (batch_size, seq_size，ffn_num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2342fe-4f4f-49ff-a7c1-2d1f72cc59ec",
   "metadata": {},
   "source": [
    "# encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a23051-7cdd-40f1-beb3-60991d797623",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, embedding_size, query_size, key_size, value_size, output_size, num_heads, normalized_shape, ffn_num_hiddens, dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(embedding_size, query_size, key_size, value_size, output_size, num_heads, dropout, use_bias)\n",
    "        self.addnorm1 = AddNorm(normalized_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(output_size, ffn_num_hiddens, embedding_size)\n",
    "        self.addnorm2 = AddNorm(normalized_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        # print(self.attention(X, valid_lens).shape)\n",
    "        Y = self.addnorm1(X, self.attention(X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9ff0b-e46e-4e45-9184-cc52ade604d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "query_size = 32\n",
    "key_size = 32\n",
    "value_size = 32\n",
    "output_size = 512\n",
    "num_heads = 8\n",
    "\n",
    "ffn_num_hiddens = 200\n",
    "dropout = 0.5\n",
    "\n",
    "seq_size = 50\n",
    "\n",
    "normalized_shape = [seq_size, output_size]\n",
    "\n",
    "encoder_blk = EncoderBlock(embedding_size, query_size, key_size, value_size, output_size, num_heads, normalized_shape, ffn_num_hiddens, dropout)\n",
    "X = torch.rand(2, seq_size, embedding_size)\n",
    "o = encoder_blk(X, None)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b4af9e-74ae-484f-984d-598cb22f960d",
   "metadata": {},
   "source": [
    "EncoderBlock中的参数有：embedding_size, query_size, key_size, value_size, output_size, num_heads, normalized_shape, ffn_num_hiddens   \n",
    "其中需要注意的有：\n",
    "1. embedding_size = output_size\n",
    "2. query_size*num_heads = key_size*num_heads = value_size*num_heads = output_size\n",
    "3. normalized_shap = \\[seq_size, output_size\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34766b6-73af-4a0b-9c14-5046a6d7c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer编码器\"\"\"\n",
    "    def __init__(self, seq_size, embedding_size, query_size, key_size, value_size, output_size, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(seq_size, embedding_size)\n",
    "        self.pos_encoding = PositionalEncoding(embedding_size, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers): # 堆叠 num_layers 个 EncoderBlock\n",
    "            self.blks.add_module(\"block\"+str(i), \n",
    "                                 EncoderBlock(embedding_size, query_size, key_size, value_size, output_size, num_heads, normalized_shape, ffn_num_hiddens, dropout, use_bias))\n",
    "\n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        # 因为位置编码值在-1和1之间，因此需要嵌入值乘以嵌入维度的平方根进行缩放，然后再与位置编码相加。\n",
    "        # 第一个编码块的输入\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.embedding_size)) # 大佬的解释：token是one-hot，经过embedding相当于从词嵌入矩阵W中取特定行，而W被 Xavier初始化，其方差和嵌入维数成反比。也就是嵌入维数越大，方差越小，权重越集中于0，后续再和positional encoding相加，词嵌入特征由于绝对值太小，可能被位置信息掩盖，难以影响模型后续计算。因此需要放大W的方差，最直接的方法就是乘以维度的平方根。\n",
    "        # self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens) # 通过编码块\n",
    "            # self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb12b94-32c9-4696-a58f-549dde060b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "query_size = 32\n",
    "key_size = 32\n",
    "value_size = 32\n",
    "output_size = 512\n",
    "num_heads = 8\n",
    "\n",
    "ffn_num_hiddens = 200\n",
    "num_layers = 8\n",
    "dropout = 0.5\n",
    "\n",
    "seq_size = 50\n",
    "\n",
    "normalized_shape = [seq_size, output_size]\n",
    "\n",
    "transencoder = TransformerEncoder(seq_size, embedding_size, query_size, key_size, value_size, output_size, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout)\n",
    "X = torch.ones((2, seq_size), dtype=torch.long)\n",
    "o = transencoder(X, None)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8db37e-aeba-45d8-847d-48e2e4e2d973",
   "metadata": {},
   "source": [
    "# decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ce947-b466-4f38-9c55-b588a3279cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中第i个块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention1 = MultiHeadAttention(embedding_size, query_size, key_size, value_size, output_size, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(normalized_shape, dropout)\n",
    "        self.attention2 = MultiHeadAttention(embedding_size, query_size, key_size, value_size, output_size, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(normalized_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens, ffn_num_outputs)\n",
    "        self.addnorm3 = AddNorm(normalized_shape, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        encoding_outputs, encoding_valid_lens = state[0], state[1] # encoder的输出为decoder的输入\n",
    "        # 训练阶段，输出序列的所有词元都在同一时间处理，因此state[2][self.i]初始化为None。\n",
    "        # 预测阶段，输出序列是通过词元一个接着一个解码的，因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values # 将已经生成的词元和当前时间步的输入拼接起来，构建 key_values，确保每个时间步都能访问到之前生成的所有词元。\n",
    "        \n",
    "        if self.training: # 训练阶段\n",
    "            batch_size, seq_size, _ = X.shape\n",
    "            # dec_valid_lens的size：(batch_size,seq_size)，其中每一行是[1,2,...,seq_size]\n",
    "            dec_valid_lens = torch.arange(1, seq_size + 1, device=X.device).repeat(batch_size, 1)\n",
    "        else:\n",
    "            dec_valid_lens = None # 构建dec_valid_lens，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。\n",
    "\n",
    "        # 自注意力\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        # 编码器－解码器注意力。\n",
    "        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0e7e1-738c-44b2-9088-323cae640eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679e3a44-59f4-44c7-a05a-b65ba0c68ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50525c1a-019c-42a5-b2a7-250bc6a63f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff620fd-e48f-4b1d-b0c4-b52e9fc92545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8af210-9106-4461-bfaf-c297cfa873bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6c0b01-b0b0-4330-a3a0-6d45e2ecbbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11c6dd-aacc-4b29-96b7-2ae940f24fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d719d-79a3-4d19-8862-601cdfaf7a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec4fed-4de3-4851-b10a-5bae55ac00c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9666e-ffc3-43c2-974b-97582944c11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c49a2-8489-4542-978f-d0304bc9252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "  [1, 0, 1, 0], # Input 1\n",
    "  [0, 2, 0, 2], # Input 2\n",
    "  [1, 1, 1, 1]  # Input 3\n",
    " ]\n",
    "x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc893f-facd-441f-b0d1-b254a701cf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_W_KQV(embedding_size, W_K_size, W_Q_size,  W_V_size):\n",
    "    \"\"\" 注意：W_K_size = W_Q_size，因为后面需要计算注意力得分，必须保证K*Q.T是合理的 \"\"\"\n",
    "    # 输出为词嵌入，maybe：(seq_size, embedding_size)\n",
    "    W_K = torch.normal(0, 1, (embedding_size, W_K_size), device = device) * 0.01\n",
    "    W_Q = torch.normal(0, 1, (embedding_size, W_Q_size), device = device) * 0.01\n",
    "    W_V = torch.normal(0, 1, (embedding_size, W_V_size), device = device) * 0.01\n",
    "\n",
    "    # 附加梯度\n",
    "    params = [W_K, W_Q, W_V]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f757162-8cdb-4b73-bd17-97e6eecc280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parms = initialize_W_KQV(4, 4, 4, 4)\n",
    "parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11169e1f-af11-404a-880d-c696e9921964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_QKV(X, parms):\n",
    "    \"\"\" parms = [W_Q, W_K, W_V] \"\"\"\n",
    "    W_Q, W_K, W_V = parms\n",
    "    Q = torch.matmul(X, W_Q)\n",
    "    K = torch.matmul(X, W_K)\n",
    "    V = torch.matmul(X, W_V)\n",
    "    return [Q, K, V]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f41ee3-154c-4317-ad56-aa596a4412c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, K, V = get_QKV(x, parms)\n",
    "Q, K, V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be7df7-7c5d-43a7-b705-df6d66f63473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算注意力分数 - Q乘K.T\n",
    "# Q.shape, K.shape\n",
    "attention_scores = torch.matmul(Q, K.T)\n",
    "attention_scores # 输出的维度为：(seq_size, seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd15129-e627-44a7-beac-f033b5ca026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算softmax - 这里我们忽略了除以sqrt(d_k)，因为根据博客的内容，只是为了防止内积过大，这里我们暂时没有这个需求，所以直接使用softmax\n",
    "attention_scores_softmax = softmax(attention_scores, dim=-1) # dim=-1，指使行的和等于1\n",
    "attention_scores_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa99027-d07c-4f63-962d-734aa2bc17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "V.shape, attention_scores_softmax.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae607a38-d49e-4244-9d07-2a0ebd854a79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa108d9-a95d-44c5-b182-c92c1fbbef87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d12dc6-0b2c-43ee-a27c-107ec43b2c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5e26eb2-bb60-4769-972c-1a32a0619582",
   "metadata": {},
   "source": [
    "# test sth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53016b6-a038-45f6-9fcb-cb690a0b1543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(nn.Module):\n",
    "    def __init__(self, embedding_size, query_size, bias=False, **kwargs):\n",
    "        super(test, self).__init__(**kwargs)\n",
    "        self.W_q = nn.Linear(embedding_size, query_size, bias=bias)\n",
    "    def forward(self, X):\n",
    "        # print(self.W_q.shape)\n",
    "        queries = self.W_q(X) # RuntimeError: mat1 and mat2 shapes cannot be multiplied (50x512 and 128x512) 代表是 XW\n",
    "        return queries\n",
    "\n",
    "seq_size = 50\n",
    "embedding_size = 512\n",
    "query_size = 128\n",
    "\n",
    "X = torch.rand(1, seq_size, embedding_size)\n",
    "test = test(embedding_size, query_size)\n",
    "queries = test(X)\n",
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b36ee6-68c5-4ca6-84ef-3e1a5efcb8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540cd6a-6633-4638-83b9-cc88a0d27de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a103715-8352-4b75-aa06-e014ebaac5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
