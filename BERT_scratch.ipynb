{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e5f0e483-bcb0-4a0e-a99d-75f1f99a2278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "\n",
    "from transformer import EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da704cf0-e4a7-4501-9533-5a5a15f1a40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"获取输入序列的词元及其片段索引\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0和1分别标记片段A和B\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d41746-7b4d-48b5-beea-a6fc5b6668fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', 'I', 'love', 'you', '<sep>'] [0, 0, 0, 0, 0]\n",
      "['<cls>', 'I', 'love', 'you', '<sep>', 'I', 'love', 'you', 'too', '<sep>'] [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# test get_tokens_and_segments()\n",
    "tokens_a = [\"I\", \"love\",  \"you\"]\n",
    "tokens, segments = get_tokens_and_segments(tokens_a)\n",
    "print(tokens, segments)\n",
    "\n",
    "tokens_a = [\"I\", \"love\",  \"you\"]\n",
    "tokens_b = [\"I\", \"love\",  \"you\", \"too\"]\n",
    "tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "print(tokens, segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd0a27cd-2a0e-4855-a813-9ee829a43f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT编码器：\n",
    "    输出：字符串的数字表示\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, num_layers, num_heads, normalized_shape, ffn_num_hiddens, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768, **kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens) # 词元嵌入\n",
    "        self.segment_embedding = nn.Embedding(2, num_hiddens) # 段嵌入 - 要么第一段第二段\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, num_hiddens)) # 位置嵌入 - 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
    "        \n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", EncoderBlock(num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, True))\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        # 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae236e2-a419-4593-892a-207b52ac8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "vocab_size = 10000 \n",
    "num_hiddens = 768 # 768 = 16*16*3\n",
    "num_layers = 2\n",
    "num_heads = 4\n",
    "normalized_shape = [768]\n",
    "ffn_num_hiddens = 1024\n",
    "dropout = 0.2\n",
    "\n",
    "encoder = BERTEncoder(vocab_size, num_hiddens, num_layers, num_heads, normalized_shape, ffn_num_hiddens, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e6317ed-4aed-4279-9765-b78b75fa2afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "tokens = torch.randint(0, vocab_size, (2, 8))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]]) # batch_size=2\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b928abd-3b69-4486-b8c4-8a4753b8213c",
   "metadata": {},
   "source": [
    "# 掩蔽语言模型（Masked Language Modeling）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "035d6b39-d369-45d5-bd83-a0d605e52516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLM(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT的掩蔽语言模型任务:\n",
    "    输入：BERTEncoder的编码结果和用于预测的词元位置。\n",
    "    输出：这些位置的预测结果。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.LayerNorm(num_hiddens),\n",
    "                                 nn.Linear(num_hiddens, vocab_size))\n",
    "\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1] # 每个样本需要预测几个token\n",
    "        pred_positions = pred_positions.reshape(-1) # 将pred_positions变成一个一维的行向量\n",
    "        batch_size = X.shape[0]\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions) # 假设batch_size=2，num_pred_positions=3，那么batch_idx是np.array（[0,0,0,1,1,1]）\n",
    "        masked_X = X[batch_idx, pred_positions] # 需要预测的token的embed表示。test中为torch.Size([6, 768])\n",
    "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1)) # test中为torch.Size([2, 3, 768])\n",
    "        mlm_Y_hat = self.mlp(masked_X)\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c71a4b31-5936-4ccc-af4e-5a1387f29ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e0dd940-c1b8-4671-9e22-6c2d9284d9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 5, 2, 6, 1, 5]) \n",
      " tensor([0, 0, 0, 1, 1, 1]) \n",
      " torch.Size([2, 8, 768]) \n",
      " torch.Size([6, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe first element comes from encoded_X[0, 1, :].\\nThe second element comes from encoded_X[0, 5, :].\\nThe third element comes from encoded_X[0, 2, :].\\nThe fourth element comes from encoded_X[1, 6, :].\\nThe fifth element comes from encoded_X[1, 1, :].\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "pred_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
    "pred_positions = pred_positions.reshape(-1)\n",
    "\n",
    "batch_idx = torch.arange(0, 2)\n",
    "batch_idx = torch.repeat_interleave(batch_idx, 3)\n",
    "\n",
    "print(pred_positions, \"\\n\", batch_idx, \"\\n\", encoded_X.shape, \"\\n\", encoded_X[batch_idx, pred_positions].shape)\n",
    "\"\"\"\n",
    "The first element comes from encoded_X[0, 1, :].\n",
    "The second element comes from encoded_X[0, 5, :].\n",
    "The third element comes from encoded_X[0, 2, :].\n",
    "The fourth element comes from encoded_X[1, 6, :].\n",
    "The fifth element comes from encoded_X[1, 1, :].\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "072b1b25-3375-47c2-a758-02865c333aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test MaskLM()\n",
    "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669dd16e-f31e-4186-9a07-67adf700a768",
   "metadata": {},
   "source": [
    "# 下一句预测（Next Sentence Prediction）\n",
    "隐蔽语言模型解决了预测句子内部的问题，但还没有解决句子与句子之间的问题，简单来说就是模型还没有学会句子与句子之间的关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c0c466e-ac2f-4082-8afe-667e84549939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    \"\"\"BERT的下一句预测任务\"\"\"\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X的形状：(batchsize,num_hiddens)\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee11b915-b971-411f-b5b8-539a3831e6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1) # encoded_X.shape：(batch_size, seq_size, embed_size)\n",
    "# NSP的输入形状:(batchsize，num_hiddens)\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4045be79-bf72-4f0f-b631-98ceb75761f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344d80d7-32ac-458c-a89a-90b42268f885",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb9053-47bc-4a8f-8b0d-5b1137721360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModel(nn.Module):\n",
    "    \"\"\"BERT模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
    "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
    "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
    "                 hid_in_features=768, mlm_in_features=768,\n",
    "                 nsp_in_features=768):\n",
    "        super(BERTModel, self).__init__()\n",
    "        self.encoder = BERTEncoder(vocab_size, num_hiddens, num_layers, num_heads, normalized_shape, ffn_num_hiddens, dropout)\n",
    "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens), nn.Tanh())\n",
    "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
    "        self.nsp = NextSentencePred(nsp_in_features)\n",
    "\n",
    "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
    "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
    "        if pred_positions is not None:\n",
    "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
    "        else:\n",
    "            mlm_Y_hat = None\n",
    "        # 用于下一句预测的多层感知机分类器的隐藏层，0是“<cls>”标记的索引\n",
    "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
    "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a59cb-92c8-4d95-b4b1-b728d494727a",
   "metadata": {},
   "source": [
    "# 处理一下训练数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3d1c933-c1f4-4b87-a245-5b5ad39355d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "def extract_tarfile(tar_file_path, extract_path='.'):\n",
    "    with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
    "        tar.extractall(path=extract_path)\n",
    "        print(f\"解压缩完成：{tar_file_path} 到 {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d60894d2-ab9b-4db0-83a2-fca9206efabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解压缩完成：data/wikitext-2.tgz 到 data/wikitext-2\n"
     ]
    }
   ],
   "source": [
    "# 调用函数解压\n",
    "tar_file_path = 'data/wikitext-2.tgz'\n",
    "extract_path = 'data/wikitext-2'\n",
    "extract_tarfile(tar_file_path, extract_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a2b05-f1ba-451b-8e46-d648e3cca372",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/wikitext-2/train.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db3e22b7-af86-439b-8525-aa4610e659a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_wiki():\n",
    "    file_name = os.path.join(\"data/wikitext-2\", 'train.csv')\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # 大写字母转换为小写字母\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2]\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ca3b26f-8d92-436b-bb42-7e77ff9b80f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = _read_wiki()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8609d7b6-8e83-4874-a245-a6eb319c86d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "224d4e88-2607-4907-88ac-c84b8ed553c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the 2013 – 14 season was the <unk> season of competitive association football and 77th season in the football league played by york city football club , a professional football club based in york , north yorkshire , england',\n",
       " 'their 17th @-@ place finish in 2012 – 13 meant it was their second consecutive season in league two',\n",
       " 'the season ran from 1 july 2013 to 30 june 2014 .']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "21963b61-d43b-431b-af85-e6d4c0b00a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15496, 3, 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs), len(paragraphs[0]), len(paragraphs[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9476de7-25ab-4b5e-a836-33cdb8803172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa4a65e-d18e-4a60-8fef-09fd6b79fd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762fe818-5ea7-49f0-95aa-69d456486178",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
