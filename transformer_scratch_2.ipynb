{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af29d287-c4e8-4940-bf81-8379f28d6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from load_translation_data import load_data_nmt\n",
    "from visualization import TrainingVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d9cd48-0314-4daa-8ce6-4e6966d5a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 检查GPU是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee921e45-e88d-4a67-b075-117c7ba4ca9f",
   "metadata": {},
   "source": [
    "# 初始化所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c2a88d7-e18a-42ea-821f-9a204c1b8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "num_heads = 8\n",
    "dropout = 0.5\n",
    "\n",
    "batch_size = 128\n",
    "seq_size = 50\n",
    "\n",
    "normalized_shape = [num_hiddens]\n",
    "\n",
    "ffn_num_hiddens = 128\n",
    "num_layers = 2\n",
    "\n",
    "i = 0 \n",
    "\n",
    "num_epochs = 10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df6eaf-11cf-4428-93c5-38ca8066b4be",
   "metadata": {},
   "source": [
    "# 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69ea6cb6-4c25-4ff7-923b-2ab1dd82a3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 45,  96,   4,  ...,   1,   1,   1],\n",
      "        [  7,  35,   4,  ...,   1,   1,   1],\n",
      "        [ 14,  46, 138,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [ 80,  10,   4,  ...,   1,   1,   1],\n",
      "        [  7, 154,   4,  ...,   1,   1,   1],\n",
      "        [  7, 149,   4,  ...,   1,   1,   1]], dtype=torch.int32)\n",
      "X的有效长度: tensor([4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 3, 5, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 4, 5, 4, 4, 4, 4, 5, 4,\n",
      "        4, 4, 5, 4, 4, 3, 4, 4, 4, 4, 4, 3, 4, 4, 5, 4, 4, 4, 4, 4, 4, 5, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 5, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 5, 4, 4, 4])\n",
      "Y: tensor([[ 18,   0,  28,  ...,   1,   1,   1],\n",
      "        [  6,   7, 164,  ...,   1,   1,   1],\n",
      "        [ 26,  23, 150,  ...,   1,   1,   1],\n",
      "        ...,\n",
      "        [  0,   8,   4,  ...,   1,   1,   1],\n",
      "        [  6,   7,   0,  ...,   1,   1,   1],\n",
      "        [  6,   7,   0,  ...,   1,   1,   1]], dtype=torch.int32)\n",
      "Y的有效长度: tensor([5, 5, 5, 3, 4, 3, 3, 4, 5, 3, 3, 4, 5, 5, 3, 4, 4, 3, 5, 4, 3, 4, 3, 4,\n",
      "        3, 3, 4, 3, 5, 5, 5, 6, 5, 5, 3, 3, 5, 5, 5, 6, 6, 3, 6, 4, 6, 4, 4, 4,\n",
      "        3, 3, 3, 4, 6, 5, 5, 5, 4, 4, 5, 4, 5, 4, 6, 5, 5, 5, 5, 3, 4, 3, 5, 6,\n",
      "        5, 4, 6, 4, 5, 3, 5, 5, 3, 5, 5, 3, 3, 3, 5, 5, 3, 4, 5, 4, 5, 4, 5, 3,\n",
      "        4, 5, 5, 4, 4, 5, 5, 3, 7, 3, 5, 5, 5, 5, 5, 4, 3, 4, 5, 4, 5, 4, 4, 4,\n",
      "        4, 4, 5, 3, 4, 4, 5, 5])\n",
      "[('<unk>', 0), ('<pad>', 1), ('<bos>', 2), ('<eos>', 3), ('.', 4), ('!', 5), ('i', 6), (\"i'm\", 7), ('it', 8), ('go', 9)]\n",
      "[('<unk>', 0), ('<pad>', 1), ('<bos>', 2), ('<eos>', 3), ('.', 4), ('!', 5), ('je', 6), ('suis', 7), ('tom', 8), ('?', 9)]\n",
      "184 201\n"
     ]
    }
   ],
   "source": [
    "train_iter, src_vocab, tgt_vocab = load_data_nmt(batch_size, seq_size)\n",
    "\n",
    "for X, X_valid_len, Y, Y_valid_len in train_iter:\n",
    "    print('X:', X.type(torch.int32))\n",
    "    print('X的有效长度:', X_valid_len)\n",
    "    print('Y:', Y.type(torch.int32))\n",
    "    print('Y的有效长度:', Y_valid_len)\n",
    "    break\n",
    "\n",
    "print(list(src_vocab.token_to_idx.items())[:10]), print(list(tgt_vocab.token_to_idx.items())[:10])\n",
    "\n",
    "src_vocab_size = len(src_vocab)\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "print(src_vocab_size, tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b5067b-486d-4a22-829c-aec3b0cc0c22",
   "metadata": {},
   "source": [
    "# 位置编码 - position encoding\n",
    "$PE(pos,2i) = sin(\\dfrac{pos}{10000^{\\dfrac{2i}{d}}})$，$PE(pos,2i+1) = cos(\\dfrac{pos}{10000^{\\dfrac{2i}{d}}})$        \n",
    "pos表示token在句子中的位置，d代表词嵌入的维度，2i代表在词嵌入维度中的第几维  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1086619d-a8ef-4b00-bb70-603a60bf1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_size, dropout, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 创建一个足够长的P\n",
    "        self.P = torch.zeros((1, max_len, embedding_size)) \n",
    "        \n",
    "        numerator = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1) # 定义分子 - 转变为(max_len, 1)\n",
    "        denominator = torch.pow(10000, torch.arange(0, embedding_size, 2, dtype=torch.float32) / embedding_size) # 定义分母 - 输出维度为(1, embedding_size/2)\n",
    "        fraction = numerator / denominator # 输出维度(max_len, embedding_size/2)\n",
    "        \n",
    "        self.P[:, :, 0::2] = torch.sin(fraction) # 0::2 表示从索引 0 开始，每隔两个元素选择一个元素\n",
    "        self.P[:, :, 1::2] = torch.cos(fraction) # 1::2 表示从索引 1 开始，每隔两个元素选择一个元素。\n",
    "\n",
    "    def forward(self, X):\n",
    "        # 输入维度为：(1, seq_size, embedding_size)\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        X = self.dropout(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d44f011-e38c-4827-959a-3c2a50a263ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 50, 256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "X = torch.rand(batch_size, seq_size, num_hiddens)\n",
    "X = pos_encoding(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4687b042-2859-431e-9d10-04959a9afc94",
   "metadata": {},
   "source": [
    "# 自注意力 - 处理了掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3e68790-9b76-4534-99f2-f3c533b93bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_qkv(X, num_heads):\n",
    "    \"\"\"为了多注意力头的并行计算而变换形状\"\"\"\n",
    "    # 输入X的形状:(batch_size，seq_size，#_size*num_heads)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1) # (batch_size，seq_size，num_heads，#_size) \n",
    "    X = X.permute(0, 2, 1, 3) # (batch_size，num_heads，seq_size, #_size)\n",
    "    X = X.reshape(-1, X.shape[2], X.shape[3]) # 最终输出的形状: (batch_size*num_heads, seq_size, #_size)\n",
    "    return X\n",
    "\n",
    "def transpose_output(X, num_heads):\n",
    "    \"\"\"逆转transpose_qkv函数的操作\"\"\"\n",
    "    # 输出维度：(batch_size*num_heads, seq_size, value_size)\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2]) # (batch_size, num_heads, seq_size, value_size)\n",
    "    X = X.permute(0, 2, 1, 3) # (batch_size, seq_size, num_heads, value_size)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], -1) # (batch_size, seq_size, num_heads*value_size)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5f9cf2e6-9fb5-4b82-a60b-83218eb6928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # 由于encoder和decoder都包含了多头注意力，所以我们需要考虑掩码Masked的情况\n",
    "    def __init__(self, num_hiddens, num_heads, dropout, bias=False, **kwargs): \n",
    "        # 参数我们一般选取 $p_q h = p_k h = p_v h = p_o$，也就是说 query_size*num_heads = key_size*num_heads = value_size*num_heads = output_size\n",
    "        # 由于 Add & Norm 需要“self.dropout(Y) + X”，所以我们在  Add & Norm 中输入的 X和Y 的维度要匹配，Y是经过多头注意力之后的输出，也就是说 output_size = embedding_size\n",
    "        # 所以我们设置：num_hiddens = embedding_size = query_size*num_heads = key_size*num_heads = value_size*num_heads = output_size\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.key_size = num_hiddens / num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(num_hiddens, num_hiddens, bias=bias) # 实际上：(embedding_size, query_size*num_heads)\n",
    "        self.W_k = nn.Linear(num_hiddens, num_hiddens, bias=bias) # 实际上：(embedding_size, key_size*num_heads)\n",
    "        self.W_v = nn.Linear(num_hiddens, num_hiddens, bias=bias) # 实际上：(embedding_size, value_size*num_heads)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias) # 实际上：(value_size*num_heads, output_size)\n",
    "\n",
    "    def forward(self, X_queries, X_keys, X_values, valid_lens=None):\n",
    "\n",
    "        queries = self.W_q(X_queries) # (batch_size, seq_size, query_size*num_heads)\n",
    "        keys = self.W_k(X_keys) # (batch_size, seq_size, key_size*num_heads)\n",
    "        values = self.W_v(X_values) # (batch_size, seq_size, value_size*num_heads)\n",
    "\n",
    "        # 根据我们平板上的推导，我们一开始的想法是错误的\n",
    "        # 在Q K.T之前我们需要对 QKV 进行处理，使得第三个维度 #_size*num_heads 中的 num_heads 到第一个维度 batch_size上去，不要影响 Z_i 的结果\n",
    "        queries = transpose_qkv(queries, self.num_heads) # (batch_size*num_heads, seq_size, query_size)\n",
    "        keys = transpose_qkv(keys, self.num_heads) # (batch_size*num_heads, seq_size, key_size)\n",
    "        values = transpose_qkv(values, self.num_heads) # (batch_size*num_heads, seq_size, value_size)\n",
    "\n",
    "        # 缩放点积注意力\n",
    "        # 为了计算 Q K^T 我们需要先reshape keys\n",
    "        keys = keys.transpose(1, 2) # 交换keys的第一个和第二个维度（从0开始） -> (batch_size*num_heads, key_size, seq_size)\n",
    "        scores = torch.bmm(queries, keys) # 1.输出维度：(batch_size*num_heads, seq_size, seq_size) 2.我们需要保证 query_size = key_size！ 3.在decoder中是(batch_size*num_heads, seq_size, seq_size*(i+1))\n",
    "        scores = scores / torch.tensor([math.sqrt(self.key_size)]).to(scores.device) # 除以 根号下(key_size)；输出维度：(batch_size*num_heads, seq_size, seq_size)\n",
    "        \n",
    "        # 在点积得分矩阵上应用掩码，以便在特定位置设置为负无穷大。\n",
    "        # valid_lens的形状为：1.(batch_size,) - 可以理解为行向量，元素个数等于batch_size；2.(batch_size, seq_size)其中每一行都是[1,2,...,seq_size]\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = valid_lens.repeat_interleave(self.num_heads, dim=0) # (1, self.num_heads*batch_size) 或 将每行复制self.num_heads次(batch_size*self.num_heads, seq_size)\n",
    "            # 第一种情况：还是行向量，每个元素重复self.num_heads次；第二种情况\n",
    "            # print(\"位置1\", scores.shape, valid_lens.shape) # 位置1 torch.Size([24, 50, 50]) torch.Size([24, 50]\n",
    "            attention_weights = self.masked_softmax(scores, valid_lens)\n",
    "        else:\n",
    "            attention_weights = self.masked_softmax(scores, valid_lens)\n",
    "\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # 使得<pad>填充字符的注意力为0，所以我们需要在乘V之前进行掩码\n",
    "        attention_outputs = torch.bmm(attention_weights, values) # (batch_size*num_heads, seq_size, seq_size) 与 (batch_size*num_heads, seq_size, value_size) 相乘 -> (batch_size*num_heads, seq_size, value_size)\n",
    "\n",
    "        # 组合多头。此时的Z的维度为：(batch_size*num_heads, seq_size, value_size)，我们需要进行一定的转化\n",
    "        attention_outputs_concat = transpose_output(attention_outputs, self.num_heads) # Z_concat的维度：(batch_size, seq_size, num_heads*value_size)\n",
    "\n",
    "        outputs = self.W_o(attention_outputs_concat) # outputs的维度：(batch_size, seq_size, num_hiddens)\n",
    "        return outputs\n",
    "\n",
    "    def masked_softmax(self, scores, valid_lens):\n",
    "        # 输入 scores 的维度：(batch_size*num_heads, seq_size, seq_size)\n",
    "        # 输入 valid_lens 的维度：1.(1, self.num_heads*batch_size) 行向量；2.(batch_size*self.num_heads, seq_size)\n",
    "        shape = scores.shape # shape[-1] 为 seq_size\n",
    "        if valid_lens is None:\n",
    "            return nn.functional.softmax(scores, dim=-1) # dim=-1指定了延最后一个维度进行softmax，也就是行\n",
    "        else: \n",
    "            if valid_lens.dim() == 1:\n",
    "                mask = torch.arange(shape[-1], device=scores.device)[None, :] < valid_lens[:, None] \n",
    "            else:\n",
    "                mask = torch.arange(shape[-1], device=scores.device)[None, :] < valid_lens\n",
    "            # print(\"###\", scores.shape, valid_lens.shape)\n",
    "            # mask的维度：1.第一种情况：(self.num_heads*batch_size, seq_size)；2.第二种情况：\n",
    "            scores[~mask] = float('-inf') # scores的维度：(batch_size*num_heads, seq_size, seq_size)\n",
    "            # print(scores)\n",
    "            return nn.functional.softmax(scores, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd58f50a-ba24-46e7-8668-9ca71706d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_X = torch.tensor([[ 7, 33,  4,  3,  1],\n",
    "#                       [ 7, 35,  3,  1,  1]])\n",
    "# embedding_layer = nn.Embedding(src_vocab_size, num_hiddens)\n",
    "# input_X = embedding_layer(raw_X)\n",
    "# valid_lens = torch.tensor([4, 3])\n",
    "\n",
    "# # mha = MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "# # outputs = mha(input_X, input_X, input_X, valid_lens)\n",
    "# # print(outputs.shape)  # 应输出 (batch_size, seq_len, num_hiddens)\n",
    "\n",
    "# # encoder\n",
    "# print(\"初始的valid_lens为：\", valid_lens)\n",
    "# valid_lens = valid_lens.repeat_interleave(num_heads, dim=0)\n",
    "# print(\"第一次变换\", valid_lens)\n",
    "# valid_lens = valid_lens[:, None] \n",
    "# print(\"第二次变换\", valid_lens)\n",
    "# print(\"torch.arange(seq_size)[None, :]为：\", torch.arange(seq_size)[None, :])\n",
    "# mask = torch.arange(seq_size)[None, :] < valid_lens\n",
    "# print(mask)\n",
    "# scores = torch.rand(batch_size*num_heads, seq_size, seq_size)\n",
    "# scores[~mask] = float('-inf') # scores的维度：(batch_size*num_heads, seq_size, seq_size)\n",
    "# print(scores)\n",
    "\n",
    "# # decoder\n",
    "# dec_valid_lens = torch.arange(1, seq_size+1)\n",
    "# print(\"\\n初始的dec_valid_lens为：\", dec_valid_lens)\n",
    "# dec_valid_lens = dec_valid_lens.repeat_interleave(num_heads, dim=0)\n",
    "# print(\"第一次变换\", dec_valid_lens)\n",
    "# dec_valid_lens = dec_valid_lens[:, None] \n",
    "# print(\"第二次变换\", dec_valid_lens)\n",
    "# print(\"torch.arange(seq_size)[None, :]为：\", torch.arange(seq_size)[None, :])\n",
    "# mask = torch.arange(seq_size)[None, :] < dec_valid_lens\n",
    "# print(mask)\n",
    "# scores = torch.rand(batch_size*num_heads, seq_size, seq_size)\n",
    "# scores[~mask] = float('-inf') # scores的维度：(batch_size*num_heads, seq_size, seq_size)\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59117753-994c-4498-a3a1-c759ef26a5ad",
   "metadata": {},
   "source": [
    "#### 具体过程\n",
    "1. **掩码创建**：\n",
    "   ```python\n",
    "   mask = torch.arange(shape[-1], device=scores.device)[None, :] < valid_lens[:, None]\n",
    "   ```\n",
    "   - `torch.arange(shape[-1], device=scores.device)` 创建一个从 0 到 `shape[-1]-1` 的序列，形状为 `(seq_len,)`。\n",
    "   - `[None, :]` 将其扩展为 `(1, seq_len)`。\n",
    "   - `valid_lens[:, None]` 将 `valid_lens` 扩展为 `(batch_size * num_heads, 1)`。\n",
    "   - 比较操作 `<` 生成一个形状为 `(batch_size * num_heads, seq_len)` 的布尔掩码，其中有效位置为 `True`，无效位置为 `False`。\n",
    "\n",
    "2. **掩码应用**：\n",
    "   ```python\n",
    "   scores[~mask] = float('-inf')\n",
    "   ```\n",
    "   - `~mask` 将布尔掩码取反。\n",
    "   - 将无效位置的分数设置为 `-inf`，这样这些位置在计算 softmax 时会非常接近于零。\n",
    "\n",
    "3. **计算 softmax**：\n",
    "   ```python\n",
    "   return nn.functional.softmax(scores, dim=-1)\n",
    "   ```\n",
    "   - 对掩码后的分数应用 softmax，只对有效位置的分数进行归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b73f27-8050-4144-988b-3eac4b4e2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "mha = MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "\n",
    "queries = torch.rand(batch_size, seq_size, num_hiddens)\n",
    "keys = torch.rand(batch_size, seq_size, num_hiddens)\n",
    "values = torch.rand(batch_size, seq_size, num_hiddens)\n",
    "\n",
    "\n",
    "valid_lens = torch.tensor([seq_size, seq_size]) # 两个batch(句子)的有效长度都为seq_size\n",
    "# valid_lens = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 前向传播\n",
    "outputs = mha(queries, keys, values, valid_lens)\n",
    "print(outputs.shape)  # 应输出 (batch_size, seq_len, num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2755e295-5faf-44be-bd92-63946d46a3cc",
   "metadata": {},
   "source": [
    "# Add & Norm - 不改变输出的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d996e507-29cc-4ccb-bbdc-694ec57fab91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层规范化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        # 输入维度：(batch_size, seq_size, num_hiddens)\n",
    "        # normalized_shape是最后一个维度的大小，LN就是对每个样本的所有特征进行归一化，在transformer中就是对每个句子，也就是batch进行LN\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        fx_add_x = self.dropout(Y) + X # 残差连接\n",
    "        outputs = self.ln(fx_add_x) # Layer Normalization层归一化 - 对每个样本的所有特征进行归一化\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313173b-941c-4f8c-bff1-c8e9106cc0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "add_norm = AddNorm(normalized_shape, dropout)\n",
    "\n",
    "add_norm(queries, outputs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0997b-505b-4609-b470-485e9270b356",
   "metadata": {},
   "source": [
    "# Feed Forward 逐位前反馈神经网络 - （Position-wise Feed-Forward Network, 简称 FFN）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25390dbb-6570-4c23-a00c-d0ecd8a80899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self, num_hiddens, ffn_num_hiddens, **kwargs):\n",
    "        # 输入的维度：(batch_size, seq_size, num_hiddens)\n",
    "        # ffn_num_input = ffn_num_outputs = num_hiddens\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(num_hiddens, ffn_num_hiddens)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dense2 = nn.Linear(ffn_num_hiddens, num_hiddens)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.dense1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.dense2(X)\n",
    "        return X # (batch_size, seq_len, num_hiddens) -> (batch_size, seq_size, num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acaf307-7d3a-4a7a-9af8-69d9647fccdb",
   "metadata": {},
   "source": [
    "# encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c127744d-5b7a-472b-9a5b-b73a89e4c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer编码器块\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, use_bias=False, **kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        self.attention = MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(normalized_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(num_hiddens, ffn_num_hiddens)\n",
    "        self.addnorm2 = AddNorm(normalized_shape, dropout)\n",
    "\n",
    "    def forward(self, X, valid_lens):\n",
    "        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))\n",
    "        return self.addnorm2(Y, self.ffn(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e4efb99-b7fc-4a68-80a5-42964d8234e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer编码器\"\"\"\n",
    "    def __init__(self, src_vocab_size, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embedding_size = num_hiddens\n",
    "        self.embedding = nn.Embedding(src_vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers): # 堆叠 num_layers 个 EncoderBlock\n",
    "            self.blks.add_module(\"block\"+str(i), EncoderBlock(num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, use_bias))\n",
    "\n",
    "    def forward(self, X, valid_lens, *args):\n",
    "        # 因为位置编码值在-1和1之间，因此需要嵌入值乘以嵌入维度的平方根进行缩放，然后再与位置编码相加。\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.embedding_size)) # 大佬的解释：token是one-hot，经过embedding相当于从词嵌入矩阵W中取特定行，而W被 Xavier初始化，其方差和嵌入维数成反比。也就是嵌入维数越大，方差越小，权重越集中于0，后续再和positional encoding相加，词嵌入特征由于绝对值太小，可能被位置信息掩盖，难以影响模型后续计算。因此需要放大W的方差，最直接的方法就是乘以维度的平方根。\n",
    "        # self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens) # 通过编码块\n",
    "            # self.attention_weights[i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b981204-5a3e-45f9-a708-1f7d723daec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "encoder = TransformerEncoder(src_vocab_size, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout)\n",
    "\n",
    "raw_X = torch.randint(0, src_vocab_size, (batch_size, seq_size))\n",
    "outputs = encoder(raw_X, valid_lens)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4ccbe9-ed37-4944-8f5f-37f08744cc86",
   "metadata": {},
   "source": [
    "# decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0715c41-bb54-4523-a133-28d10fc475cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中第i个块\"\"\"\n",
    "    def __init__(self, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, i, **kwargs):\n",
    "        super(DecoderBlock, self).__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention1 = MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(normalized_shape, dropout)\n",
    "        self.attention2 = MultiHeadAttention(num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(normalized_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(num_hiddens, ffn_num_hiddens)\n",
    "        self.addnorm3 = AddNorm(normalized_shape, dropout)\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1] # encoder的输出为decoder的输入\n",
    "        # state[2] 是一个列表，长度与解码器层数相同。用于存储每个解码器块在解码过程中已经生成的词元表示。在预测阶段，这些表示将用于生成后续的词元。\n",
    "        # 训练阶段，输出序列的所有词元都在同一时间处理，因此state[2][self.i]初始化为None。\n",
    "        # 预测阶段，输出序列是通过词元一个接着一个解码的，因此state[2][self.i]包含着直到当前时间步第i个解码块的输出表示\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else: \n",
    "            key_values = torch.cat((state[2][self.i], X), axis=1) # (batch_size, seq_size, num_hiddens) -> (batch_size, seq_size*(i+1), num_hiddens)\n",
    "        state[2][self.i] = key_values # 将已经生成的词元和当前时间步的输入拼接起来，构建 key_values，确保每个时间步都能访问到之前生成的所有词元。\n",
    "\n",
    "        if self.training: \n",
    "            batch_size, seq_size, _ = X.shape\n",
    "            dec_valid_lens = torch.arange(1, seq_size + 1, device=X.device).repeat(batch_size, 1) # repeat把：(1, seq_size) -> (batch_size, seq_size)，其中每一行都是[1,2,...,seq_size]\n",
    "        else: \n",
    "            dec_valid_lens = None # 构建dec_valid_lens，以便任何查询都只会与解码器中所有已经生成词元的位置（即直到该查询位置为止）进行注意力计算。\n",
    "\n",
    "        # 自注意力\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)\n",
    "        Y = self.addnorm1(X, X2)\n",
    "        # 编码器－解码器注意力。\n",
    "        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens) # 可以理解\n",
    "        Z = self.addnorm2(Y, Y2)\n",
    "        return self.addnorm3(Z, self.ffn(Z)), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965eebe9-7dad-4b67-8aed-1a4486467f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "encoder_blk = EncoderBlock(num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout)\n",
    "\n",
    "decoder_blk = DecoderBlock(num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, i)\n",
    "decoder_blk.eval()\n",
    "X = torch.ones((batch_size, seq_size, num_hiddens))\n",
    "state = [encoder_blk(X, valid_lens), valid_lens, [None]]\n",
    "decoder_blk(X, state)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24b35990-c596-44f3-a734-8d79b53aa337",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embedding_size = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(tgt_vocab_size, num_hiddens)\n",
    "        self.pos_encoding = PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\"block\"+str(i), DecoderBlock(num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, dropout, i))\n",
    "        self.dense = nn.Linear(num_hiddens, tgt_vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        state = [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "        return state\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.embedding_size))\n",
    "        # self._attention_weights = [[None] * len(self.blks) for _ in range (2)]\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            print(X)\n",
    "            X, state = blk(X, state)\n",
    "            # 解码器自注意力权重\n",
    "            # self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # “编码器－解码器”自注意力权重\n",
    "            # self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53089dd9-11d1-4701-a2a6-d2635be54aa1",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7bf45708-45d8-412b-8aff-580595f56bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_X, dec_X, *args):\n",
    "        enc_outputs = self.encoder(enc_X, *args)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args)\n",
    "        return self.decoder(dec_X, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df649e05-d21d-4b63-af28-bf2a178fc970",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(src_vocab_size, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout)\n",
    "decoder = TransformerDecoder(tgt_vocab_size, num_hiddens, num_heads, normalized_shape, ffn_num_hiddens, num_layers, dropout)\n",
    "\n",
    "model = EncoderDecoder(encoder, decoder) # 输入 - X, dec_input, X_valid_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8bf53370-f4ad-4e3f-af9b-aba507af887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm # 整体进行一个缩放"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a40b297-145b-4b0b-be15-5d78492e7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_len, value=0): # valid_length 有效长度\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    maxlen = X.size(1) # 获取第二个维度的大小\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)\\\n",
    "                                                                        [None, :] < valid_len[:, None]\n",
    "    # [None, :]：将该一维张量扩展为形状 (1, maxlen)\n",
    "    # valid_len[:, None]：将有效长度张量 valid_len 从形状 (batch_size,) 扩展为 (batch_size, 1)\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带遮蔽的softmax交叉熵损失函数\"\"\"\n",
    "    # pred的形状：(batch_size,num_steps,vocab_size)\n",
    "    # label的形状：(batch_size,num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label) # 创建一个与指定张量具有相同形状和相同数据类型的新张量，并将所有元素初始化为 1\n",
    "        weights = sequence_mask(weights, valid_len) # 根据valid_len将weights中的部分设置为0\n",
    "        self.reduction='none'\n",
    "        unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label) \n",
    "        '''\n",
    "        调用父类 nn.CrossEntropyLoss 的 forward 方法计算未加权的损失。\n",
    "        由于 nn.CrossEntropyLoss 期望输入形状为 (batch_size, num_classes, num_steps)，所以我们通过 pred.permute(0, 2, 1) 将 pred 的形状从 (batch_size, num_steps, vocab_size) 转换为 (batch_size, vocab_size, num_steps)\n",
    "        '''\n",
    "        # print(unweighted_loss.shape)：(batch_size,num_steps)\n",
    "        # 将未加权的损失 unweighted_loss 与权重 weights 相乘，对每个时间步进行加权。然后对 dim=1 维度（时间步维度）进行平均，得到每个序列的加权损失。\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56fe3005-791b-464c-9f31-496873c3de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, data_iter, lr, num_epochs, tgt_vocab, device):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    \n",
    "    def xavier_init_weights(m): # 使用 Xavier 均匀分布初始化 Transformer 模型中的权重。\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    model.apply(xavier_init_weights)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = MaskedSoftmaxCELoss()\n",
    "    model.train()\n",
    "    visualizer = TrainingVisualizer(xlabel='Epoch', ylabel='Value', title='Train transformer_scratch', legend=['Train Loss'])\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for X, X_valid_len, Y, Y_valid_len in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "            X, X_valid_len, Y, Y_valid_len = X.to(device), X_valid_len.to(device), Y.to(device), Y_valid_len.to(device)\n",
    "            \n",
    "            bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1) # Y.shape[0]为batch_size，\n",
    "            dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学。dim=0，加行；dim=1，加列。Y[:, :-1]选择所有行，列从第一列到倒数第二列\n",
    "            \n",
    "            Y_hat, _ = model(X, dec_input, X_valid_len) # 编码器输入：(X, X_valid_len)；解码器输入：(dec_input, 初始化的state)\n",
    "            # print(Y_hat)\n",
    "            loss = criterion(Y_hat, Y, Y_valid_len)\n",
    "            # print(loss)\n",
    "            loss.sum().backward()\t\n",
    "            grad_clipping(model, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            optimizer.step()\n",
    "            \n",
    "        visualizer.add(epoch, [total_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f01c60ad-f083-4881-aede-0eaf5e6aa041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.8017e+01, -0.0000e+00,  4.9094e+01,  ..., -1.8611e+01,\n",
      "           5.5491e+00,  0.0000e+00],\n",
      "         [-1.7508e+01, -0.0000e+00, -1.4171e+01,  ...,  0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [-1.6210e+01, -4.5225e+00,  1.8140e+01,  ..., -0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 1.4482e+01, -2.2278e+01, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 1.2698e+01, -2.1574e+01, -2.1293e+01,  ...,  0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 1.2327e+01, -0.0000e+00, -2.0560e+01,  ...,  6.4132e+01,\n",
      "          -6.1915e-02, -7.7595e+01]],\n",
      "\n",
      "        [[ 2.8017e+01, -7.4447e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
      "           5.5491e+00,  1.2294e+01],\n",
      "         [-0.0000e+00, -0.0000e+00,  0.0000e+00,  ..., -1.0601e+01,\n",
      "          -4.2028e-01,  0.0000e+00],\n",
      "         [-0.0000e+00, -0.0000e+00,  2.1097e-01,  ...,  0.0000e+00,\n",
      "          -3.7458e+01, -4.9409e+00],\n",
      "         ...,\n",
      "         [ 1.4482e+01, -2.2278e+01, -2.3044e+01,  ...,  6.4132e+01,\n",
      "          -6.2345e-02, -0.0000e+00],\n",
      "         [ 0.0000e+00, -2.1574e+01, -0.0000e+00,  ...,  6.4132e+01,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 1.2327e+01, -1.9692e+01, -2.0560e+01,  ...,  6.4132e+01,\n",
      "          -6.1915e-02, -7.7595e+01]],\n",
      "\n",
      "        [[ 2.8017e+01, -7.4447e+00,  4.9094e+01,  ..., -0.0000e+00,\n",
      "           5.5491e+00,  0.0000e+00],\n",
      "         [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  2.8479e+01,\n",
      "           6.9901e+00,  5.1491e+01],\n",
      "         [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
      "          -0.0000e+00,  6.7635e+01],\n",
      "         ...,\n",
      "         [ 0.0000e+00, -2.2278e+01, -2.3044e+01,  ...,  6.4132e+01,\n",
      "          -0.0000e+00, -7.7595e+01],\n",
      "         [ 0.0000e+00, -0.0000e+00, -0.0000e+00,  ...,  6.4132e+01,\n",
      "          -0.0000e+00, -7.7595e+01],\n",
      "         [ 1.2327e+01, -1.9692e+01, -2.0560e+01,  ...,  6.4132e+01,\n",
      "          -6.1915e-02, -7.7595e+01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0000e+00, -7.4447e+00,  4.9094e+01,  ..., -0.0000e+00,\n",
      "           5.5491e+00,  0.0000e+00],\n",
      "         [-2.1551e+01,  0.0000e+00, -0.0000e+00,  ...,  1.3438e+01,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 2.4844e+01,  5.0846e+01, -4.0525e+00,  ..., -9.5170e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00, -2.2278e+01, -0.0000e+00,  ...,  0.0000e+00,\n",
      "          -0.0000e+00, -7.7595e+01],\n",
      "         [ 0.0000e+00, -0.0000e+00, -2.1293e+01,  ...,  0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 1.2327e+01, -1.9692e+01, -2.0560e+01,  ...,  6.4132e+01,\n",
      "          -6.1915e-02, -0.0000e+00]],\n",
      "\n",
      "        [[ 2.8017e+01, -7.4447e+00,  4.9094e+01,  ..., -1.8611e+01,\n",
      "           5.5491e+00,  0.0000e+00],\n",
      "         [-0.0000e+00,  6.1560e+01, -5.9487e+00,  ...,  0.0000e+00,\n",
      "           1.2672e+01,  6.1399e+01],\n",
      "         [-0.0000e+00, -4.5225e+00,  1.8140e+01,  ..., -0.0000e+00,\n",
      "           0.0000e+00,  5.0271e+01],\n",
      "         ...,\n",
      "         [ 1.4482e+01, -0.0000e+00, -2.3044e+01,  ...,  6.4132e+01,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 1.2698e+01, -0.0000e+00, -2.1293e+01,  ...,  0.0000e+00,\n",
      "          -6.2130e-02, -0.0000e+00],\n",
      "         [ 0.0000e+00, -1.9692e+01, -0.0000e+00,  ...,  6.4132e+01,\n",
      "          -0.0000e+00, -0.0000e+00]],\n",
      "\n",
      "        [[ 0.0000e+00, -0.0000e+00,  4.9094e+01,  ..., -1.8611e+01,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 4.9378e+01, -0.0000e+00,  4.5673e+01,  ..., -6.5518e+01,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -9.5170e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00, -2.2278e+01, -2.3044e+01,  ...,  0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 0.0000e+00, -2.1574e+01, -0.0000e+00,  ...,  6.4132e+01,\n",
      "          -0.0000e+00, -0.0000e+00],\n",
      "         [ 1.2327e+01, -1.9692e+01, -2.0560e+01,  ...,  0.0000e+00,\n",
      "          -0.0000e+00, -0.0000e+00]]], device='cuda:0',\n",
      "       grad_fn=<NativeDropoutBackward0>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         ...,\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
      "\n",
      "        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [-52.7716,   0.0756,   0.0000,  ...,  -0.0000,   4.2912,  22.2895],\n",
      "         ...,\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
      "\n",
      "        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         ...,\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         ...,\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
      "\n",
      "        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         ...,\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]],\n",
      "\n",
      "        [[     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         ...,\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan],\n",
      "         [     nan,      nan,      nan,  ...,      nan,      nan,      nan]]],\n",
      "       device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], device='cuda:0',\n",
      "       grad_fn=<NativeDropoutBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_seq2seq(model, train_iter, lr, num_epochs, tgt_vocab, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c8913-3206-421f-b22c-9692999e9fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a9bae-a9f4-4637-b6b1-07b3d1d001a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4610a433-5c70-4ed5-9b6d-7d3f28896556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
